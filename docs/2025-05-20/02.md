You've got a good grasp of the client proxy mechanism! Now, let's look at the rest of the codebase for similar opportunities.

**Overall Impression:**

The codebase is generally well-structured. The use of `Options` structs for configuration, centralized `shared_types`, and robust `testutil` are all strong points. The Go `websocket` library (a fork of `nhooyr.io/websocket`) is modern and context-aware, which is beneficial.

The primary areas for potential improvement often revolve around:

1.  **Reducing Boilerplate in Handler Invocation:** The reflection necessary to call user-defined handlers with typed payloads can be verbose.
2.  **Clarifying Complex Interactions:** Modules like `hotreload` interact with several components (broker, filewatcher, JS client), and ensuring these interactions are clear and direct is key.
3.  **Enhancing Type Safety at API Boundaries:** Similar to the client proxy, ensuring that developers using the library get strong type guarantees where possible.

Here are some specific areas and suggestions:

**1. Handler Invocation (Broker-side and Client-side)**

*   **Current State:** Both `broker.go` (`handleClientRequest`) and `client.go` (`invokeSubscriptionHandler`, `invokeClientRequestHandler`) use reflection to:
    1.  Create an instance of the expected request/message type.
    2.  Unmarshal the `json.RawMessage` payload into this instance.
    3.  Prepare the `reflect.Value` for the handler call (handling if the handler expects a value or a pointer).
    4.  Call the handler.
*   **Opportunity for DRY:** This sequence of reflective operations is repeated.
*   **Suggestion:** Introduce a helper function within the `ergosockets` package to encapsulate the payload decoding and argument preparation.

    ```go
    // File: pkg/ergosockets/common.go (or a new reflect_helpers.go)
    // ...

    // DecodeAndPrepareArg decodes a JSON payload into an instance of targetType
    // and returns a reflect.Value suitable for calling a handler function.
    // It handles whether targetType is a pointer or a value.
    func DecodeAndPrepareArg(payload json.RawMessage, targetType reflect.Type) (reflect.Value, error) {
        var instanceVal reflect.Value // This will be a pointer to the actual type

        // Create a new instance of the target type.
        // If targetType is UserStruct, reflect.New(targetType) gives *UserStruct.
        // If targetType is *UserStruct, reflect.New(targetType.Elem()) gives *UserStruct.
        if targetType.Kind() == reflect.Ptr {
            instanceVal = reflect.New(targetType.Elem())
        } else {
            instanceVal = reflect.New(targetType)
        }

        // Unmarshal into the pointer.
        if payload != nil && string(payload) != "null" { // Handle null/empty payload
            if err := json.Unmarshal(payload, instanceVal.Interface()); err != nil {
                return reflect.Value{}, fmt.Errorf("failed to unmarshal payload into %s: %w. Raw: %s", targetType, err, string(payload))
            }
        }

        // If the handler expects a pointer (*UserStruct), pass the instanceVal directly.
        // If the handler expects a value (UserStruct), pass the dereferenced element.
        if targetType.Kind() == reflect.Ptr {
            return instanceVal, nil
        }
        return instanceVal.Elem(), nil
    }
    ```

    This helper could then be used in `broker.go` and `client.go` to simplify the handler invocation logic. For example, in `broker.go`'s `handleClientRequest`:

    ```go
    // Before: (simplified)
    // var reqPayloadVal reflect.Value
    // if handlerWrapper.ReqType.Kind() == reflect.Ptr { ... } else { ... }
    // if err := json.Unmarshal(reqEnv.Payload, reqPayloadVal.Interface()); err != nil { ... }
    // var inputs []reflect.Value
    // inputs = append(inputs, reflect.ValueOf(mc))
    // if handlerWrapper.ReqType.Kind() == reflect.Ptr { inputs = append(inputs, reqPayloadVal) } else { inputs = append(inputs, reqPayloadVal.Elem())}
    // results := handlerWrapper.HandlerFunc.Call(inputs)

    // After:
    reqArg, err := ergosockets.DecodeAndPrepareArg(reqEnv.Payload, handlerWrapper.ReqType)
    if err != nil {
        // ... handle error, send error envelope ...
        mc.logger.Info(fmt.Sprintf("Broker: Failed to decode request payload for topic '%s': %v", reqEnv.Topic, err))
        errEnv, _ := ergosockets.NewEnvelope(reqEnv.ID, ergosockets.TypeError, reqEnv.Topic, nil,
            &ergosockets.ErrorPayload{Code: http.StatusBadRequest, Message: "Invalid request payload: " + err.Error()})
        mc.trySend(errEnv)
        return
    }
    inputs := []reflect.Value{reflect.ValueOf(mc), reqArg} // mc is the ClientHandle
    results := handlerWrapper.HandlerFunc.Call(inputs)
    // ... (rest of the logic for processing results) ...
    ```

**2. HotReload Service Client Error Handling**

*   **Current State:** `HotReload.Start()` has a section commented "Also set up a subscription to handle published client errors... This is needed because the JavaScript client uses publish...". However, the Go code that follows `hr.broker.HandleClientRequest(TopicClientError, ...)` sets up a *request handler*. The subsequent `IterateClients` call sending a message on `TopicClientError` from server to client doesn't make the server subscribe to client-published errors.
*   **Clarity & Consistency Issue:** If the JavaScript `HotReload.Client` *publishes* errors to `system:client_error`, the Go `HotReload` service needs to act as a subscriber to that topic on the broker. If the JS client *sends a request* for error reporting, the current `HandleClientRequest` is correct.
*   **Suggestion:**
    1.  **Verify JavaScript Client Behavior:** Determine definitively if `hotreload.js` uses `client.publish()` or `client.sendServerRequest()` for errors.
    2.  **Align Go and JS:**
        *   **If JS publishes:** The `HotReload` service needs a mechanism to receive these published messages. This is non-trivial because `HotReload` is not a standard `client.Client` that can call `cli.Subscribe()`.
            *   One way is for `HotReload` to internally create a `client.Client` instance that connects to its own broker (loopback) and subscribes.
            *   Alternatively, and perhaps more cleanly, the broker could offer an internal API for services like `HotReload` to register as a "topic listener" without being a full WebSocket client. This is a larger broker change.
        *   **If JS sends a request (Recommended for Simplicity):** The current `hr.broker.HandleClientRequest(TopicClientError, ...)` in `HotReload.Start()` is correct. The JS client should use `sendServerRequest("system:client_error", errorPayload)`. The confusing comment and the `IterateClients` call in `HotReload.Start` (related to error subscription) should be removed or significantly reworded to reflect its actual purpose (if any remains).
            Making the JS client send a request for error reporting seems more idiomatic and simpler to integrate with the current Go broker/handler structure.

    Assuming the path of making the JS client send a *request* for error reporting:

    ```go
    // File: pkg/hotreload/hotreload.go
    // In func (hr *HotReload) Start()

    // Remove or clarify this block:
    /*
    // Also set up a subscription to handle published client errors
    // This is needed because the JavaScript client uses publish instead of request
    hr.broker.IterateClients(func(client broker.ClientHandle) bool {
        // Subscribe to the client error topic
        client.Send(context.Background(), TopicClientError, nil) // This does NOT make the server subscribe. It sends a message TO clients.
        return true
    })
    */

    // The following existing code is correct IF the JS client sends a REQUEST to TopicClientError:
    err := hr.broker.HandleClientRequest(TopicClientError, func(client broker.ClientHandle, payload map[string]interface{}) error {
        hr.handleClientError(client.ID(), payload)
        return nil // Acknowledge the request (no response payload needed for error reporting)
    })
    if err != nil {
        // It's important to log/return this error, as it means a core part of hotreload isn't working
        hr.logger.Error("Failed to register client error handler with broker", "error", err)
        return fmt.Errorf("failed to setup client error handler: %w", err)
    }
    ```
    And the comment in `hotreload_test.go` for the custom error handler would be: "The JS client will send a *request* to `system:client_error`..."

**3. Broker `removeClient` and Client `Close` Coordination**

*   **Current State:** When a client disconnects or is removed:
    *   `broker.removeClient()` calls `mc.cancel()` first, then removes from maps, then closes the connection.
    *   `managedClient.readPump()`'s defer calls `mc.broker.removeClient(mc)`.
    *   `managedClient.writePump()`'s defer logs, but `removeClient` is primarily triggered by `readPump` or `pingLoop` failures.
    *   `client.Client.Close()` cancels its main context, waits for pumps, then closes the connection.
*   **Clarity/Robustness:** The interaction of context cancellations and explicit `conn.Close()` calls needs to be robust to avoid race conditions or redundant operations. The current use of `mc.ctx` (derived from broker's main context) and `client.clientCtx` / `client.currentConnPumpCtx` seems generally correct for signaling goroutines to stop.
*   **Specific in `managedClient.writePump`:**
    ```go
    //NOT SURE IF THIS IS CORRECT
    mc.conn.CloseRead(context.Background())
    // if websocket.CloseStatus() == -1 {
    // 	mc.conn.Close(websocket.StatusGoingAway, "client context cancelled")
    // }
    ```
    `CloseRead` is fine. The commented-out `websocket.CloseStatus()` check and subsequent `mc.conn.Close()` might be redundant if `removeClient` (called by `readPump`'s defer, which should also exit due to `mc.ctx.Done()`) handles the main connection closure. The goal is that one primary path closes the WebSocket connection and signals others. `mc.cancel()` in `removeClient` should ensure all client-specific goroutines observe `mc.ctx.Done()` and terminate.
*   **Suggestion:** Ensure that `mc.cancel()` is consistently the primary signal for a `managedClient`'s goroutines to stop, and that one designated place (likely `removeClient` after `mc.cancel()` or the read/write pump itself upon detecting an unrecoverable error) is responsible for the `websocket.Conn.Close()` call. The current logic seems to lean this way but the "NOT SURE IF THIS IS CORRECT" comment indicates a point of potential refinement in understanding the exact shutdown sequence. `CloseRead` is often used to unblock the read side while allowing the write side to finish sending queued messages before a full `Close`.

**4. `testutil/broker.go` - `NewBrokerServer` Ready Channel**

*   `ready := make(chan struct{}, 1); ready <- struct{}{}`. This is fine for immediate readiness. If there were async setup in `NewBrokerServer` before it's truly usable, the `ready` channel would be closed *after* that setup. Current use is okay.

**5. Test Helper `TestBrowserConnection` in `pkg/browser_client/browser_tests/test_helpers.go`**

*   It checks status text for "Connected". This is good.
*   It iterates `bs.IterateClients` to find the browser client.
*   It checks `page.GetCurrentURL()` for `client_id`.
*   **Suggestion:** This helper is quite comprehensive. One minor improvement could be to make the "Connected" status text configurable or to wait for a specific JavaScript variable/event that signals full client-side initialization beyond just the WebSocket connection, if applicable for more complex tests. For now, it serves its purpose well.

**General Code Style and Practices:**

*   **Error Handling:** Generally good, with errors wrapped and propagated.
*   **Logging:** `slog` is used, which is good. Log messages are informative.
*   **Concurrency:** Mutexes seem to be used appropriately to protect shared data in both broker and client. `sync.RWMutex` is used where applicable.
*   **Constants:** Centralized topic constants in `shared_types` is excellent.

**No Major Architectural Flaws Found (Post-Proxy Review)**

After the client proxy redesign, the rest of the codebase doesn't reveal architectural issues of similar magnitude. The suggestions above are more about refining existing patterns for better maintainability (DRY for reflection) and ensuring clarity in specific interactions (hotreload error handling). The foundation is solid.

I will provide the suggested `DecodeAndPrepareArg` helper and show its integration into one of the handler invocation sites as an example. I'll also provide the cleaned-up `HotReload.Start()` assuming the JS client will be changed to send requests for error reporting.

**File: pkg/ergosockets/common.go**
```go
// ergosockets/common.go
package ergosockets

import (
	"crypto/rand"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"reflect"
	"time"
)

// Cached error type for reflection.
var ErrType = reflect.TypeOf((*error)(nil)).Elem()

// GenerateID creates a new random hex string ID.
func GenerateID() string {
	bytes := make([]byte, 16)
	if _, err := rand.Read(bytes); err != nil {
		return fmt.Sprintf("fallback-%x", reflect.ValueOf(TimeNow()).Int()) // TimeNow from time.go
	}
	return hex.EncodeToString(bytes)
}

// HandlerWrapper stores reflection info for invoking user-provided handlers.
type HandlerWrapper struct {
	HandlerFunc reflect.Value // The user's handler function
	ReqType     reflect.Type  // Type of the request payload struct (for request handlers)
	RespType    reflect.Type  // Type of the response payload struct (for request handlers that return one)
	MsgType     reflect.Type  // Type of the message payload (for subscription handlers)
}

// NewHandlerWrapper inspects the handler function and extracts type information.
// Supported signatures:
// Server HandleClientRequest: func(ClientHandle, ReqStruct) (RespStruct, error)
// Server HandleClientRequest: func(ClientHandle, ReqStruct) error
// Client Subscribe: func(MsgStruct) error
// Client HandleServerRequest: func(ReqStruct) (RespStruct, error)
// Client HandleServerRequest: func(ReqStruct) error
func NewHandlerWrapper(handlerFunc interface{}) (*HandlerWrapper, error) {
	fv := reflect.ValueOf(handlerFunc)
	ft := fv.Type()

	if ft.Kind() != reflect.Func {
		return nil, fmt.Errorf("handler must be a function, got %T", handlerFunc)
	}

	hw := &HandlerWrapper{HandlerFunc: fv}
	numIn := ft.NumIn()
	numOut := ft.NumOut()

	// Check common error return (last output arg must be error)
	if numOut == 0 || !ft.Out(numOut-1).Implements(ErrType) {
		return nil, fmt.Errorf("handler must return an error as its last argument, signature: %s", ft.String())
	}

	// Client Subscribe: func(MsgStruct) error
	if numIn == 1 && numOut == 1 {
		hw.MsgType = ft.In(0)
		// Ensure MsgStruct is not an interface or pointer to interface, etc.
		// For simplicity, we assume it's a concrete type or pointer to struct.
		if hw.MsgType.Kind() == reflect.Interface || (hw.MsgType.Kind() == reflect.Ptr && hw.MsgType.Elem().Kind() == reflect.Interface) {
			return nil, fmt.Errorf("subscription handler message type cannot be an interface: %s", ft.String())
		}
		return hw, nil
	}

	// Client HandleServerRequest: func(ReqStruct) (RespStruct, error) or func(ReqStruct) error
	if numIn == 1 && (numOut == 1 || numOut == 2) {
		hw.ReqType = ft.In(0)
		if hw.ReqType.Kind() == reflect.Interface || (hw.ReqType.Kind() == reflect.Ptr && hw.ReqType.Elem().Kind() == reflect.Interface) {
			return nil, fmt.Errorf("client HandleServerRequest handler request type cannot be an interface: %s", ft.String())
		}
		if numOut == 2 { // func(ReqStruct) (RespStruct, error)
			hw.RespType = ft.Out(0)
			if hw.RespType.Kind() == reflect.Interface || (hw.RespType.Kind() == reflect.Ptr && hw.RespType.Elem().Kind() == reflect.Interface) {
				return nil, fmt.Errorf("client HandleServerRequest handler response type cannot be an interface: %s", ft.String())
			}
		}
		return hw, nil
	}

	// Server HandleClientRequest: func(ClientHandle, ReqStruct) (RespStruct, error) or func(ClientHandle, ReqStruct) error
	if numIn == 2 && (numOut == 1 || numOut == 2) {
		// First arg should be ClientHandle (interface) - we don't strictly check its type here
		// as it's passed by the broker itself.
		hw.ReqType = ft.In(1)
		if hw.ReqType.Kind() == reflect.Interface || (hw.ReqType.Kind() == reflect.Ptr && hw.ReqType.Elem().Kind() == reflect.Interface) {
			return nil, fmt.Errorf("server HandleClientRequest handler request type cannot be an interface: %s", ft.String())
		}
		if numOut == 2 { // func(ClientHandle, ReqStruct) (RespStruct, error)
			hw.RespType = ft.Out(0)
			if hw.RespType.Kind() == reflect.Interface || (hw.RespType.Kind() == reflect.Ptr && hw.RespType.Elem().Kind() == reflect.Interface) {
				return nil, fmt.Errorf("server HandleClientRequest handler response type cannot be an interface: %s", ft.String())
			}
		}
		return hw, nil
	}

	return nil, fmt.Errorf("unsupported handler signature: %s. Expected e.g., func(ch ClientHandle, req ReqT) (RespT, error), func(msg MsgT) error, or func(req ReqT) (RespT, error)", ft.String())
}

// TimeNow is a wrapper for time.Now, useful for testing if time needs to be mocked.
// For this implementation, we'll use the real time.Now().
var TimeNow = time.Now

// DecodeAndPrepareArg decodes a JSON payload into an instance of targetType
// and returns a reflect.Value suitable for calling a handler function.
// It handles whether targetType is a pointer or a value.
func DecodeAndPrepareArg(payload json.RawMessage, targetType reflect.Type) (reflect.Value, error) {
	var instanceVal reflect.Value // This will be a pointer to the actual type

	// Create a new instance of the target type.
	// If targetType is UserStruct, reflect.New(targetType) gives *UserStruct.
	// If targetType is *UserStruct, reflect.New(targetType.Elem()) gives *UserStruct.
	if targetType.Kind() == reflect.Ptr {
		instanceVal = reflect.New(targetType.Elem())
	} else {
		instanceVal = reflect.New(targetType)
	}

	// Unmarshal into the pointer.
	if payload != nil && string(payload) != "null" { // Handle null/empty payload
		if err := json.Unmarshal(payload, instanceVal.Interface()); err != nil {
			return reflect.Value{}, fmt.Errorf("failed to unmarshal payload into %s: %w. Raw: %s", targetType, err, string(payload))
		}
	}

	// If the handler expects a pointer (*UserStruct), pass the instanceVal directly.
	// If the handler expects a value (UserStruct), pass the dereferenced element.
	if targetType.Kind() == reflect.Ptr {
		return instanceVal, nil
	}
	return instanceVal.Elem(), nil
}
```
**File: pkg/broker/broker.go**
```go
// ergosockets/broker/broker.go
package broker

import (
	"context"
	"encoding/json" // Keep for system:proxy response, other direct JSON ops if any
	"errors"
	"fmt"
	"log/slog"
	"net/http"
	"reflect"
	"sync"
	"time"

	"github.com/coder/websocket"
	"github.com/coder/websocket/wsjson"
	"github.com/lightforgemedia/go-websocketmq/pkg/ergosockets"
	"github.com/lightforgemedia/go-websocketmq/pkg/shared_types"
)

const (
	defaultClientSendBuffer     = 16 // Refined: Added for slow client policy
	defaultWriteTimeout         = 10 * time.Second
	defaultReadTimeout          = 60 * time.Second // Should be > ping interval if pings enabled
	libraryDefaultPingInterval  = 30 * time.Second // Library's own default if user passes 0 to WithPingInterval
	defaultServerRequestTimeout = 10 * time.Second
)

type brokerConfig struct {
	logger                *slog.Logger // Refined: Logger interface
	acceptOptions         *websocket.AcceptOptions
	clientSendBuffer      int // Refined: For outgoing messages per client
	writeTimeout          time.Duration
	readTimeout           time.Duration
	pingInterval          time.Duration // 0 means use libraryDefaultPingInterval, <0 means disable
	serverRequestTimeout  time.Duration
	serveJavaScriptClient bool // Whether to serve the JavaScript client
}

// Broker manages client connections and message routing.
type Broker struct {
	config brokerConfig

	clientsMu      sync.RWMutex
	managedClients map[string]*managedClient // clientID -> client
	sessionIndexMu sync.RWMutex
	sessionIndex   map[string]*managedClient // client provided ID -> client

	requestHandlersMu sync.RWMutex
	requestHandlers   map[string]*ergosockets.HandlerWrapper // topic -> handler

	publishSubscribersMu sync.RWMutex
	publishSubscribers   map[string]map[*managedClient]struct{} // topic -> set of clients

	shutdownOnce sync.Once
	shutdownChan chan struct{}   // Closed when broker starts shutting down
	mainCtx      context.Context // Top-level context for the broker itself
	mainCancel   context.CancelFunc
}

// Functional options have been removed in favor of the Options pattern.
// Use broker.NewWithOptions() with broker.Options struct instead.

// The broker.New() function has been removed. Use broker.NewWithOptions() instead.

// UpgradeHandler returns an http.HandlerFunc to handle WebSocket upgrade requests.
func (b *Broker) UpgradeHandler() http.HandlerFunc {
	return func(w http.ResponseWriter, r *http.Request) {
		select {
		case <-b.shutdownChan: // Or <-b.mainCtx.Done()
			http.Error(w, "server is shutting down", http.StatusServiceUnavailable)
			b.config.logger.Info("Broker: Rejected connection, server shutting down.")
			return
		default:
		}

		conn, err := websocket.Accept(w, r, b.config.acceptOptions)
		if err != nil {
			b.config.logger.Info(fmt.Sprintf("Broker: Failed to accept websocket connection: %v", err))
			return
		}

		// Get client ID from URL (client-provided ID)
		clientProvidedID := r.URL.Query().Get("client_id")
		if clientProvidedID == "" {
			// If not provided, generate a temporary one
			clientProvidedID = ergosockets.GenerateID()
		}

		// Generate a server-assigned ID (source of truth)
		// serverAssignedID := ergosockets.GenerateID()

		// Get client URL if available
		clientURL := ""
		referer := r.Header.Get("Referer")
		if referer != "" {
			clientURL = referer
		}

		// Client's context is derived from broker's main context
		clientCtx, clientCancel := context.WithCancel(b.mainCtx)

		// Default client name based on ID or URL
		clientName := "client-" + clientProvidedID[:8]
		if clientURL != "" {
			clientName = "browser-" + clientURL
		}

		mc := &managedClient{
			id:                    clientProvidedID,
			clientID:              clientProvidedID,
			name:                  clientName,
			clientType:            "unknown", // Will be updated during registration
			clientURL:             clientURL,
			conn:                  conn,
			broker:                b,
			send:                  make(chan *ergosockets.Envelope, b.config.clientSendBuffer),
			ctx:                   clientCtx,
			cancel:                clientCancel,
			activeSubscriptions:   make(map[string]struct{}),
			pendingServerRequests: make(map[string]chan *ergosockets.Envelope),
			logger:                b.config.logger, // Pass logger to managedClient
			registered:            false,           // Will be set to true after registration
		}

		b.addClient(mc)
		mc.logger.Info(fmt.Sprintf("Broker: Client %s connected", mc.id))

		go mc.writePump()
		go mc.readPump()
		if b.config.pingInterval > 0 { // Only start ping loop if interval is positive
			go mc.pingLoop()
		}
	}
}

func (b *Broker) addClient(mc *managedClient) {
	b.clientsMu.Lock()
	defer b.clientsMu.Unlock()
	b.managedClients[mc.id] = mc
}

func (b *Broker) removeClient(mc *managedClient) {
	mc.cancel() // Signal all client-specific goroutines to stop FIRST

	b.clientsMu.Lock()
	if _, exists := b.managedClients[mc.id]; !exists {
		b.clientsMu.Unlock() // Already removed
		return
	}
	delete(b.managedClients, mc.id)
	b.clientsMu.Unlock()

	b.publishSubscribersMu.Lock()
	mc.activeSubscriptionsMu.Lock() // Lock client's subs before iterating
	for topic := range mc.activeSubscriptions {
		if subs, ok := b.publishSubscribers[topic]; ok {
			delete(subs, mc)
			if len(subs) == 0 {
				delete(b.publishSubscribers, topic)
			}
		}
	}
	mc.activeSubscriptionsMu.Unlock()
	b.publishSubscribersMu.Unlock()

	b.sessionIndexMu.Lock()
	if cur, ok := b.sessionIndex[mc.clientID]; ok && cur == mc {
		delete(b.sessionIndex, mc.clientID)
	}
	b.sessionIndexMu.Unlock()

	// Close the connection after removing from maps and cancelling context
	// This ensures no new messages are queued to its send channel after this point.
	// StatusPolicyViolation might have been set by writePump if it was a slow client.
	// Otherwise, use StatusNormalClosure or StatusGoingAway.
	mc.conn.CloseRead(context.Background())
	// currentStatus := websocket.CloseStatus() // Check if already closed with a specific status
	// if currentStatus == -1 {                 // Not yet closed or unknown status
	// 	mc.conn.Close(websocket.StatusNormalClosure, "client removed")
	// }

	mc.logger.Info(fmt.Sprintf("Broker: Client %s disconnected and removed.", mc.id))
}

// HandleClientRequest registers a handler for a specific request topic from clients.
// handlerFunc must be of type: func(ClientHandle, ReqStruct) (RespStruct, error) or func(ClientHandle, ReqStruct) error
func (b *Broker) HandleClientRequest(topic string, handlerFunc interface{}) error {
	hw, err := ergosockets.NewHandlerWrapper(handlerFunc)
	if err != nil {
		return fmt.Errorf("broker HandleClientRequest topic '%s': %w", topic, err)
	}
	if hw.HandlerFunc.Type().NumIn() != 2 {
		return fmt.Errorf("broker HandleClientRequest topic '%s': handler must have 2 input arguments (ClientHandle, RequestType), got %d", topic, hw.HandlerFunc.Type().NumIn())
	}

	b.requestHandlersMu.Lock()
	defer b.requestHandlersMu.Unlock()
	if _, exists := b.requestHandlers[topic]; exists {
		return fmt.Errorf("broker: handler already registered for topic '%s'", topic)
	}
	b.requestHandlers[topic] = hw
	b.config.logger.Info(fmt.Sprintf("Broker: Registered request handler for topic '%s'", topic))
	return nil
}

// Publish sends a message to all clients subscribed to the given topic.
func (b *Broker) Publish(ctx context.Context, topic string, payloadData interface{}) error {
	select {
	case <-b.mainCtx.Done(): // Use broker's main context for shutdown check
		return errors.New("broker is shutting down")
	default:
	}

	env, err := ergosockets.NewEnvelope("", ergosockets.TypePublish, topic, payloadData, nil)
	if err != nil {
		return fmt.Errorf("broker: failed to create publish envelope for topic '%s': %w", topic, err)
	}

	b.publishSubscribersMu.RLock()
	// Create a snapshot of subscribers to avoid holding lock during send attempts
	subscribersToNotify := make([]*managedClient, 0)
	if subs, ok := b.publishSubscribers[topic]; ok {
		for mc := range subs {
			subscribersToNotify = append(subscribersToNotify, mc)
		}
	}
	b.publishSubscribersMu.RUnlock()

	if len(subscribersToNotify) == 0 {
		// b.config.logger.Info(fmt.Sprintf("Broker: No subscribers for publish to topic '%s'", topic) // Can be noisy
		return nil
	}

	b.config.logger.Info(fmt.Sprintf("Broker: Publishing message on topic '%s' to %d subscribers", topic, len(subscribersToNotify)))
	for _, mc := range subscribersToNotify {
		// Use trySend which will count dropped messages and disconnect slow clients
		mc.trySend(env)
	}
	return nil
}

// GetClient retrieves a handle to a connected client by its ID.
func (b *Broker) GetClient(clientID string) (ClientHandle, error) {
	b.clientsMu.RLock()
	defer b.clientsMu.RUnlock()
	mc, ok := b.managedClients[clientID]
	if !ok {
		return nil, fmt.Errorf("client with ID '%s' not found", clientID)
	}
	return mc, nil
}

// GetClientBySessionID retrieves a handle to a connected client by its client-provided ID.
func (b *Broker) GetClientBySessionID(sessionID string) (ClientHandle, error) {
	b.sessionIndexMu.RLock()
	mc, ok := b.sessionIndex[sessionID]
	b.sessionIndexMu.RUnlock()
	if !ok {
		return nil, fmt.Errorf("client with session ID '%s' not found", sessionID)
	}
	return mc, nil
}

// Shutdown gracefully shuts down the broker.
func (b *Broker) Shutdown(ctx context.Context) error {
	b.shutdownOnce.Do(func() {
		b.config.logger.Info("Broker: Initiating shutdown...")
		close(b.shutdownChan) // Signal internal components that rely on this
		b.mainCancel()        // Cancel the broker's main context, which propagates to clients

		// Wait for all client goroutines (read/write/ping pumps) to finish.
		// This requires managedClients to have their own WaitGroup or similar.
		// For now, we'll rely on the context cancellation and a short wait.
		// A more robust shutdown would involve each managedClient signaling its completion.

		b.clientsMu.RLock()
		numClients := len(b.managedClients)
		b.clientsMu.RUnlock()
		b.config.logger.Info(fmt.Sprintf("Broker: Waiting for %d clients to disconnect...", numClients))

		// Simple wait loop, a more robust system would use a WaitGroup for clients.
		// Or check against b.mainCtx.Done() if clients are guaranteed to stop.
		// The client's mc.cancel() in removeClient should ensure their pumps stop.
		// The removeClient calls happen as client read/write pumps exit due to context cancellation.
	})

	// Wait for a specified period or until all clients are gone (simplified)
	timeout := time.NewTimer(5 * time.Second) // Max wait for clients to clear up
	defer timeout.Stop()
	ticker := time.NewTicker(100 * time.Millisecond)
	defer ticker.Stop()

	for {
		b.clientsMu.RLock()
		remainingClients := len(b.managedClients)
		b.clientsMu.RUnlock()
		if remainingClients == 0 {
			b.config.logger.Info("Broker: All clients disconnected.")
			break
		}
		select {
		case <-timeout.C:
			b.config.logger.Info(fmt.Sprintf("Broker: Shutdown timed out waiting for %d clients.", remainingClients))
			return errors.New("broker shutdown timed out")
		case <-ticker.C:
			// continue waiting
		case <-ctx.Done(): // External shutdown context timed out
			b.config.logger.Info(fmt.Sprintf("Broker: External shutdown context timed out (%d clients remaining): %v", remainingClients, ctx.Err()))
			return ctx.Err()
		}
	}

	b.config.logger.Info("Broker: Shutdown complete.")
	return nil
}

// Context returns the broker's main context, which is cancelled on Shutdown.
func (b *Broker) Context() context.Context {
	return b.mainCtx
}

// --- managedClient (internal representation of a connected client) ---
type managedClient struct {
	id         string // Server-assigned ID (source of truth)
	clientID   string // Client-provided ID (for reference only)
	name       string // Human-readable name for the client
	clientType string // Type of client (e.g., "browser", "app", "service")
	clientURL  string // URL of the client (for browser connections)
	conn       *websocket.Conn
	broker     *Broker
	send       chan *ergosockets.Envelope // Buffered channel for outgoing messages
	logger     *slog.Logger

	ctx    context.Context    // Context for this client's lifetime, derived from broker.mainCtx
	cancel context.CancelFunc // Cancels this client's context

	activeSubscriptionsMu sync.Mutex
	activeSubscriptions   map[string]struct{}

	pendingServerRequestsMu sync.Mutex
	pendingServerRequests   map[string]chan *ergosockets.Envelope

	droppedMessagesMu sync.Mutex
	droppedMessages   int // Counter for dropped messages due to full send buffer

	registered bool // Whether the client has completed registration
}

// Methods for ClientHandle interface
func (mc *managedClient) ID() string               { return mc.id }
func (mc *managedClient) ClientID() string         { return mc.clientID }
func (mc *managedClient) Name() string             { return mc.name }
func (mc *managedClient) ClientType() string       { return mc.clientType }
func (mc *managedClient) ClientURL() string        { return mc.clientURL }
func (mc *managedClient) Context() context.Context { return mc.ctx }

func (mc *managedClient) SendClientRequest(ctx context.Context, topic string, requestData interface{}, responsePayloadPtr interface{}, timeout time.Duration) error {
	select {
	case <-mc.ctx.Done():
		return fmt.Errorf("client %s disconnected: %w", mc.id, mc.ctx.Err())
	case <-mc.broker.mainCtx.Done(): // Use broker's main context for shutdown check
		return errors.New("broker is shutting down")
	default:
	}

	correlationID := ergosockets.GenerateID()
	reqEnv, err := ergosockets.NewEnvelope(correlationID, ergosockets.TypeRequest, topic, requestData, nil)
	if err != nil {
		return fmt.Errorf("failed to create request envelope for client %s: %w", mc.id, err)
	}

	respChan := make(chan *ergosockets.Envelope, 1)
	mc.pendingServerRequestsMu.Lock()
	mc.pendingServerRequests[correlationID] = respChan
	mc.pendingServerRequestsMu.Unlock()

	defer func() {
		mc.pendingServerRequestsMu.Lock()
		delete(mc.pendingServerRequests, correlationID)
		mc.pendingServerRequestsMu.Unlock()
		// Do not close respChan here, receiver might still be selecting on it if timeout occurred on send.
		// It will be garbage collected. Or, ensure it's drained if not used.
	}()

	// Send the request
	sendCtx, sendCancel := context.WithTimeout(ctx, mc.broker.config.writeTimeout) // Timeout for the send itself
	defer sendCancel()
	select {
	case mc.send <- reqEnv:
		mc.logger.Info(fmt.Sprintf("Broker: Sent request (ID: %s) on topic '%s' to client %s", correlationID, topic, mc.id))
	case <-mc.ctx.Done():
		return fmt.Errorf("client %s context done before sending request: %w", mc.id, mc.ctx.Err())
	case <-sendCtx.Done(): // Send timed out
		return fmt.Errorf("timeout sending request to client %s (ID: %s): %w", mc.id, correlationID, sendCtx.Err())
	case <-ctx.Done(): // Overall request context timed out/cancelled
		return fmt.Errorf("requesting context done before sending request to client %s (ID: %s): %w", mc.id, correlationID, ctx.Err())
	}

	// Wait for the response
	effectiveTimeout := mc.broker.config.serverRequestTimeout
	if timeout > 0 {
		effectiveTimeout = timeout
	}

	// The timer should be based on the parent context `ctx` for the whole operation.
	timer := time.NewTimer(effectiveTimeout)
	defer timer.Stop()

	select {
	case respEnv, ok := <-respChan:
		if !ok {
			return fmt.Errorf("response channel closed for request ID %s to client %s (client likely disconnected or internal error)", correlationID, mc.id)
		}
		if respEnv.Error != nil {
			return fmt.Errorf("client %s responded with error (code %d) for request ID %s: %s", mc.id, respEnv.Error.Code, correlationID, respEnv.Error.Message)
		}
		if responsePayloadPtr != nil { // Only decode if a non-nil pointer is provided
			if reflect.ValueOf(responsePayloadPtr).IsNil() {
				// Programmer error: passed a nil pointer for decoding.
				return fmt.Errorf("responsePayloadPtr cannot be nil for request ID %s from client %s", correlationID, mc.id)
			}
			if err := respEnv.DecodePayload(responsePayloadPtr); err != nil {
				return fmt.Errorf("failed to decode response payload from client %s for request ID %s: %w", mc.id, correlationID, err)
			}
		}
		mc.logger.Info(fmt.Sprintf("Broker: Received response (ID: %s) from client %s", correlationID, mc.id))
		return nil
	case <-timer.C:
		return fmt.Errorf("request to client %s (ID: %s) timed out after %v", mc.id, correlationID, effectiveTimeout)
	case <-mc.ctx.Done():
		return fmt.Errorf("client %s context done while waiting for response (ID: %s): %w", mc.id, correlationID, mc.ctx.Err())
	case <-ctx.Done(): // Overall request context timed out/cancelled
		return fmt.Errorf("requesting context done while waiting for response from client %s (ID: %s): %w", mc.id, correlationID, ctx.Err())
	}
}

func (mc *managedClient) Send(ctx context.Context, topic string, payloadData interface{}) error {
	select {
	case <-mc.ctx.Done():
		return fmt.Errorf("client %s disconnected: %w", mc.id, mc.ctx.Err())
	case <-mc.broker.mainCtx.Done():
		return errors.New("broker is shutting down")
	default:
	}

	env, err := ergosockets.NewEnvelope("", ergosockets.TypePublish, topic, payloadData, nil)
	if err != nil {
		return fmt.Errorf("failed to create send envelope for client %s: %w", mc.id, err)
	}

	// Use trySend which will count dropped messages and disconnect slow clients
	mc.trySend(env)
	mc.logger.Info(fmt.Sprintf("Broker: Sent direct message on topic '%s' to client %s", topic, mc.id))
	return nil
}

func (mc *managedClient) readPump() {
	defer mc.broker.removeClient(mc) // Ensures cleanup on any exit

	cfg := mc.broker.config
	readDeadlineDuration := cfg.readTimeout
	if cfg.pingInterval > 0 { // If pings are enabled, base read deadline on ping interval
		readDeadlineDuration = cfg.pingInterval * 2
		if readDeadlineDuration < cfg.readTimeout { // But ensure it's not less than configured min read timeout
			readDeadlineDuration = cfg.readTimeout
		}
	}

	if readDeadlineDuration > 0 {
		mc.conn.SetReadLimit(1024 * 1024) // Max message size 1MB
		// _ = mc.conn.SetReadDeadline(ergosockets.TimeNow().Add(readDeadlineDuration))
		// mc.conn.SetPongHandler(func(string) error {
		// 	// mc.logger.Info(fmt.Sprintf("Broker: Pong received from client %s", mc.id)
		// 	if readDeadlineDuration > 0 {
		// 		_ = mc.conn.SetReadDeadline(ergosockets.TimeNow().Add(readDeadlineDuration))
		// 	}
		// 	return nil
		// })
	}

	for {
		select {
		case <-mc.ctx.Done(): // Check for client context cancellation first
			mc.logger.Info(fmt.Sprintf("Broker: Client %s readPump stopping due to context cancellation: %v", mc.id, mc.ctx.Err()))
			return
		default:
		}

		var env ergosockets.Envelope
		// Use a context for the Read operation that can be shorter than mc.ctx
		// For example, link it to the readDeadline if one is set, or just use mc.ctx
		readOpCtx := mc.ctx // For now, use client's main context for read op
		err := wsjson.Read(readOpCtx, mc.conn, &env)
		if err != nil {
			status := websocket.CloseStatus(err)
			if errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded) ||
				status == websocket.StatusNormalClosure || status == websocket.StatusGoingAway {
				mc.logger.Info(fmt.Sprintf("Broker: Client %s readPump closing gracefully: %v", mc.id, err))
			} else {
				mc.logger.Info(fmt.Sprintf("Broker: Client %s read error in readPump: %v (status: %d)", mc.id, err, status))
			}
			return // Exits loop, triggers defer removeClient
		}

		// if readDeadlineDuration > 0 { // Refresh read deadline on successful message read
		// 	_ = mc.conn.SetReadDeadline(ergosockets.TimeNow().Add(readDeadlineDuration))
		// }

		// Process envelope
		switch env.Type {
		case ergosockets.TypeRequest:
			go mc.handleClientRequest(&env) // Process in goroutine to not block readPump
		case ergosockets.TypeResponse, ergosockets.TypeError:
			mc.pendingServerRequestsMu.Lock()
			if ch, ok := mc.pendingServerRequests[env.ID]; ok {
				select {
				case ch <- &env: // Try to send, non-blocking
				default:
					mc.logger.Info(fmt.Sprintf("Broker: Response channel for ID %s (client %s) not ready (possibly timed out or already processed)", env.ID, mc.id))
				}
			} else {
				mc.logger.Info(fmt.Sprintf("Broker: Received unsolicited server-targeted response/error with ID %s from client %s", env.ID, mc.id))
			}
			mc.pendingServerRequestsMu.Unlock()
		case ergosockets.TypePublish:
			mc.handleClientPublish(&env)
		case ergosockets.TypeSubscribeRequest:
			mc.handleSubscribeRequest(&env)
		case ergosockets.TypeUnsubscribeRequest:
			mc.handleUnsubscribeRequest(&env)
		default:
			mc.logger.Info(fmt.Sprintf("Broker: Client %s sent unknown envelope type: '%s'", mc.id, env.Type))
		}
	}
}

func (mc *managedClient) handleClientRequest(reqEnv *ergosockets.Envelope) {
	mc.broker.requestHandlersMu.RLock()
	handlerWrapper, ok := mc.broker.requestHandlers[reqEnv.Topic]
	mc.broker.requestHandlersMu.RUnlock()

	if !ok {
		mc.logger.Info(fmt.Sprintf("Broker: No handler for request topic '%s' from client %s", reqEnv.Topic, mc.id))
		errEnv, _ := ergosockets.NewEnvelope(reqEnv.ID, ergosockets.TypeError, reqEnv.Topic, nil,
			&ergosockets.ErrorPayload{Code: http.StatusNotFound, Message: "No handler for topic: " + reqEnv.Topic})
		mc.trySend(errEnv)
		return
	}

	reqArg, err := ergosockets.DecodeAndPrepareArg(reqEnv.Payload, handlerWrapper.ReqType)
	if err != nil {
		mc.logger.Info(fmt.Sprintf("Broker: Failed to decode request payload for topic '%s' from client %s: %v. Payload: %s", reqEnv.Topic, mc.id, err, string(reqEnv.Payload)))
		errEnv, _ := ergosockets.NewEnvelope(reqEnv.ID, ergosockets.TypeError, reqEnv.Topic, nil,
			&ergosockets.ErrorPayload{Code: http.StatusBadRequest, Message: "Invalid request payload: " + err.Error()})
		mc.trySend(errEnv)
		return
	}

	// Prepare the input arguments for the handler function
	inputs := []reflect.Value{reflect.ValueOf(mc), reqArg} // ClientHandle, DecodedPayload

	// Call the handler function
	results := handlerWrapper.HandlerFunc.Call(inputs)

	var errResult error
	if len(results) > 0 {
		if errVal, ok := results[len(results)-1].Interface().(error); ok {
			errResult = errVal
		}
	}

	if errResult != nil {
		mc.logger.Info(fmt.Sprintf("Broker: Handler for topic '%s' (client %s) returned error: %v", reqEnv.Topic, mc.id, errResult))
		errEnv, _ := ergosockets.NewEnvelope(reqEnv.ID, ergosockets.TypeError, reqEnv.Topic, nil,
			&ergosockets.ErrorPayload{Code: http.StatusInternalServerError, Message: errResult.Error()}) // Consider mapping codes
		mc.trySend(errEnv)
		return
	}

	if handlerWrapper.RespType != nil {
		respPayload := results[0].Interface()
		respEnv, err := ergosockets.NewEnvelope(reqEnv.ID, ergosockets.TypeResponse, reqEnv.Topic, respPayload, nil)
		if err != nil {
			mc.logger.Info(fmt.Sprintf("Broker: Failed to create response envelope for topic '%s' (client %s): %v", reqEnv.Topic, mc.id, err))
			serverErrEnv, _ := ergosockets.NewEnvelope(reqEnv.ID, ergosockets.TypeError, reqEnv.Topic, nil,
				&ergosockets.ErrorPayload{Code: http.StatusInternalServerError, Message: "Server error creating response"})
			mc.trySend(serverErrEnv)
			return
		}
		mc.trySend(respEnv)
	} else if reqEnv.ID != "" { // No response payload, but request had an ID, send simple ack
		ackEnv, _ := ergosockets.NewEnvelope(reqEnv.ID, ergosockets.TypeResponse, reqEnv.Topic, nil, nil)
		mc.trySend(ackEnv)
	}
}

func (mc *managedClient) handleSubscribeRequest(env *ergosockets.Envelope) {
	topic := env.Topic
	if topic == "" {
		mc.logger.Info(fmt.Sprintf("Broker: Client %s sent subscribe request with empty topic", mc.id))
		errEnv, _ := ergosockets.NewEnvelope(env.ID, ergosockets.TypeError, "", nil, &ergosockets.ErrorPayload{Code: http.StatusBadRequest, Message: "Subscription topic cannot be empty"})
		mc.trySend(errEnv)
		return
	}

	mc.activeSubscriptionsMu.Lock()
	mc.activeSubscriptions[topic] = struct{}{}
	mc.activeSubscriptionsMu.Unlock()

	mc.broker.publishSubscribersMu.Lock()
	if _, ok := mc.broker.publishSubscribers[topic]; !ok {
		mc.broker.publishSubscribers[topic] = make(map[*managedClient]struct{})
	}
	mc.broker.publishSubscribers[topic][mc] = struct{}{}
	mc.broker.publishSubscribersMu.Unlock()

	mc.logger.Info(fmt.Sprintf("Broker: Client %s subscribed to topic '%s'", mc.id, topic))
	ackEnv, _ := ergosockets.NewEnvelope(env.ID, ergosockets.TypeSubscriptionAck, topic, map[string]string{"status": "subscribed", "topic": topic}, nil)
	mc.trySend(ackEnv)
}

func (mc *managedClient) handleUnsubscribeRequest(env *ergosockets.Envelope) {
	topic := env.Topic
	if topic == "" { // Should not happen if client validates
		mc.logger.Info(fmt.Sprintf("Broker: Client %s sent unsubscribe request with empty topic", mc.id))
		return
	}

	mc.activeSubscriptionsMu.Lock()
	delete(mc.activeSubscriptions, topic)
	mc.activeSubscriptionsMu.Unlock()

	mc.broker.publishSubscribersMu.Lock()
	if subs, ok := mc.broker.publishSubscribers[topic]; ok {
		delete(subs, mc)
		if len(subs) == 0 {
			delete(mc.broker.publishSubscribers, topic)
		}
	}
	mc.broker.publishSubscribersMu.Unlock()

	mc.logger.Info(fmt.Sprintf("Broker: Client %s unsubscribed from topic '%s'", mc.id, topic))
	// Optionally send ack for unsubscribe
	ackEnv, _ := ergosockets.NewEnvelope(env.ID, ergosockets.TypeSubscriptionAck, topic, map[string]string{"status": "unsubscribed", "topic": topic}, nil) // Re-use ack type
	mc.trySend(ackEnv)
}

func (mc *managedClient) handleClientPublish(env *ergosockets.Envelope) {
	topic := env.Topic
	if topic == "" {
		mc.logger.Info(fmt.Sprintf("Broker: Client %s sent publish with empty topic", mc.id))
		return
	}

	// Forward the publish to all subscribers
	err := mc.broker.Publish(mc.ctx, topic, env.Payload)
	if err != nil {
		mc.logger.Info(fmt.Sprintf("Broker: Failed to publish message from client %s to topic '%s': %v", mc.id, topic, err))
	} else {
		mc.logger.Info(fmt.Sprintf("Broker: Client %s published message to topic '%s'", mc.id, topic))
	}
}

// trySend attempts to send an envelope to the client's send channel without blocking indefinitely.
func (mc *managedClient) trySend(env *ergosockets.Envelope) {
	select {
	case mc.send <- env:
		// Message sent successfully
	case <-mc.ctx.Done():
		mc.logger.Info(fmt.Sprintf("Broker: Client %s context done, cannot send envelope type %s on topic %s", mc.id, env.Type, env.Topic))
	default: // Should only happen if send channel is full and writePump is also blocked/slow
		mc.logger.Info(fmt.Sprintf("Broker: Client %s send channel full when trying to send envelope type %s on topic %s. Message potentially dropped.", mc.id, env.Type, env.Topic))

		// This indicates a slow client; disconnect it after a few dropped messages
		mc.droppedMessagesMu.Lock()
		mc.droppedMessages++
		droppedCount := mc.droppedMessages
		mc.droppedMessagesMu.Unlock()

		// If we've dropped too many messages, disconnect the client
		if droppedCount >= 3 { // Threshold for disconnection
			mc.logger.Info(fmt.Sprintf("Broker: Client %s dropped %d messages, disconnecting slow client.", mc.id, droppedCount))
			// Close the connection; readPump's defer will handle full removeClient
			mc.conn.Close(websocket.StatusPolicyViolation, "too many dropped messages")
			// Also remove the client directly to ensure it's removed immediately
			go mc.broker.removeClient(mc)
		}
	}
}

func (mc *managedClient) writePump() {
	defer func() {
		// This defer ensures that if writePump exits (e.g., due to error or context cancellation),
		// it triggers the full client removal process.
		// mc.broker.removeClient(mc) // removeClient is called by readPump's defer or if ping fails
		mc.logger.Info(fmt.Sprintf("Broker: Client %s writePump stopping.", mc.id))
	}()

	for {
		select {
		case message, ok := <-mc.send:
			if !ok { // send channel closed by broker.removeClient or broker.Shutdown
				mc.logger.Info(fmt.Sprintf("Broker: Client %s send channel closed, closing connection.", mc.id))
				mc.conn.Close(websocket.StatusNormalClosure, "send channel closed")
				return
			}

			writeCtx, cancel := context.WithTimeout(mc.ctx, mc.broker.config.writeTimeout)
			err := wsjson.Write(writeCtx, mc.conn, message)
			cancel() // Release resources associated with writeCtx

			if err != nil {
				mc.logger.Info(fmt.Sprintf("Broker: Client %s write error in writePump: %v. Closing connection.", mc.id, err))
				// A write error typically means the connection is bad.
				// Close the connection; readPump's defer will handle full removeClient.
				mc.conn.Close(websocket.CloseStatus(err), "write error") // Use status from error if available
				return
			}
		case <-mc.ctx.Done(): // Client's context cancelled
			mc.logger.Info(fmt.Sprintf("Broker: Client %s context cancelled, writePump stopping.", mc.id))
			// Connection might already be closed by removeClient or pingLoop.
			// If not, close it now.
			//NOT SURE IF THIS IS CORRECT
			mc.conn.CloseRead(context.Background())

			// if websocket.CloseStatus() == -1 {
			// 	mc.conn.Close(websocket.StatusGoingAway, "client context cancelled")
			// }
			return
		}
	}
}

func (mc *managedClient) pingLoop() {
	if mc.broker.config.pingInterval <= 0 { // Guard: only run if interval is positive
		return
	}
	ticker := time.NewTicker(mc.broker.config.pingInterval)
	defer ticker.Stop()
	mc.logger.Info(fmt.Sprintf("Broker: Client %s pingLoop started with interval %v", mc.id, mc.broker.config.pingInterval))

	for {
		select {
		case <-ticker.C:
			pingCtx, cancel := context.WithTimeout(mc.ctx, mc.broker.config.pingInterval/2) // Timeout for ping op itself
			err := mc.conn.Ping(pingCtx)
			cancel()
			if err != nil {
				mc.logger.Info(fmt.Sprintf("Broker: Client %s ping failed: %v. Closing connection.", mc.id, err))
				// Ping failure means connection is likely dead. Close it.
				// removeClient will be called by readPump's defer when it detects the closure.
				mc.conn.Close(websocket.StatusPolicyViolation, "ping failure")
				return // Exit ping loop
			}
			// mc.logger.Info(fmt.Sprintf("Broker: Ping sent to client %s", mc.id)
		case <-mc.ctx.Done(): // Client's context cancelled
			mc.logger.Info(fmt.Sprintf("Broker: Client %s context cancelled, pingLoop stopping.", mc.id))
			return
		}
	}
}

// Test helper: IterateClients
func (b *Broker) IterateClients(f func(ClientHandle) bool) {
	b.clientsMu.RLock()
	snapshot := make([]ClientHandle, 0, len(b.managedClients))
	for _, client := range b.managedClients {
		snapshot = append(snapshot, client)
	}
	b.clientsMu.RUnlock()

	for _, client := range snapshot {
		if !f(client) {
			break
		}
	}
}

// setupDefaultHandlers adds the default system handlers to the broker
func (b *Broker) setupDefaultHandlers() {
	// Add default client registration handler
	err := b.HandleClientRequest(shared_types.TopicClientRegister, func(client ClientHandle, req shared_types.ClientRegistration) (shared_types.ClientRegistrationResponse, error) {
		// Get the managedClient from the ClientHandle
		mc, ok := client.(*managedClient)
		if !ok {
			return shared_types.ClientRegistrationResponse{}, fmt.Errorf("invalid client type")
		}

		// Update client information
		mc.clientID = req.ClientID
		b.sessionIndexMu.Lock()
		b.sessionIndex[mc.clientID] = mc
		b.sessionIndexMu.Unlock()

		// Set client name based on provided name or URL
		if req.ClientName != "" {
			mc.name = req.ClientName
		}

		// Set client type
		if req.ClientType != "" {
			mc.clientType = req.ClientType
		}

		// Set client URL
		if req.ClientURL != "" {
			mc.clientURL = req.ClientURL
		} else if mc.clientURL == "" && req.ClientType == "browser" {
			// For browser clients without URL, use a default
			mc.clientURL = "unknown-browser"
		}

		// Mark client as registered
		mc.registered = true

		// Log registration
		mc.logger.Info(fmt.Sprintf("Broker: Client %s registered as %s (type: %s, URL: %s)", mc.id, mc.name, mc.clientType, mc.clientURL))

		// Return response with server-assigned ID
		return shared_types.ClientRegistrationResponse{
			ServerAssignedID: mc.id,
			ClientName:       mc.name,
			ServerTime:       time.Now().Format(time.RFC3339),
		}, nil
	})

	// Log error but don't fail if registration handler can't be added
	if err != nil {
		b.config.logger.Info(fmt.Sprintf("Broker: Failed to add client registration handler: %v", err))
	}

	// Handler for client-to-client proxy requests
	err = b.HandleClientRequest(shared_types.TopicProxyRequest,
		func(src ClientHandle, req shared_types.ProxyRequest) (json.RawMessage, error) {
			dest, err := b.GetClient(req.TargetID)
			if err != nil {
				return nil, err
			}
			var resp json.RawMessage
			if err := dest.SendClientRequest(src.Context(), req.Topic, req.Payload, &resp, b.config.serverRequestTimeout); err != nil {
				return nil, err
			}
			return resp, nil
		})
	if err != nil {
		b.config.logger.Info(fmt.Sprintf("Broker: Failed to add proxy handler: %v", err))
	}

	// Handler to list connected clients
	err = b.HandleClientRequest(shared_types.TopicListClients,
		func(ch ClientHandle, req shared_types.ListClientsRequest) (shared_types.ListClientsResponse, error) {
			var list []shared_types.ClientSummary
			b.IterateClients(func(c ClientHandle) bool {
				if req.ClientType == "" || req.ClientType == c.ClientType() {
					list = append(list, shared_types.ClientSummary{
						ID:         c.ID(),
						Name:       c.Name(),
						ClientType: c.ClientType(),
						ClientURL:  c.ClientURL(),
					})
				}
				return true
			})
			return shared_types.ListClientsResponse{Clients: list}, nil
		})
	if err != nil {
		b.config.logger.Info(fmt.Sprintf("Broker: Failed to add list clients handler: %v", err))
	}
}

```
**File: pkg/client/client.go**
```go
// ergosockets/client/client.go
package client

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"log/slog"
	"math/rand" // For jitter
	"net/http"
	"reflect"
	"strings"
	"sync"
	"time"

	"github.com/coder/websocket"
	"github.com/coder/websocket/wsjson"
	"github.com/lightforgemedia/go-websocketmq/pkg/ergosockets"
	"github.com/lightforgemedia/go-websocketmq/pkg/shared_types"
)

const (
	defaultClientSendBuffer   = 16 // Matches broker's managedClient
	defaultClientReqTimeout   = 10 * time.Second
	defaultWriteClientTimeout = 5 * time.Second
	defaultReadClientTimeout  = 60 * time.Second // Should be > server ping interval
	// Client-initiated pings are disabled by default. Rely on server pings.
	libraryDefaultClientPingInterval = 0 * time.Second
	defaultReconnectAttempts         = 0 // 0 means no auto-reconnect by default
	defaultReconnectDelayMin         = 1 * time.Second
	defaultReconnectDelayMax         = 30 * time.Second
)

type clientConfig struct {
	logger                *slog.Logger
	dialOptions           *websocket.DialOptions
	defaultRequestTimeout time.Duration // Renamed from defaultTimeout
	writeTimeout          time.Duration
	readTimeout           time.Duration
	pingInterval          time.Duration // Client-initiated pings; 0 or <0 to disable
	autoReconnect         bool
	reconnectAttempts     int // 0 for infinite if autoReconnect is true
	reconnectDelayMin     time.Duration
	reconnectDelayMax     time.Duration
	clientName            string
	clientType            string
	clientURL             string
}

// Client is a WebSocket client for ErgoSockets.
type Client struct {
	config clientConfig
	urlStr string
	id     string // Unique ID for this client instance/connection session

	conn   *websocket.Conn
	connMu sync.RWMutex

	send chan *ergosockets.Envelope

	// Overall client lifetime context
	clientCtx    context.Context
	clientCancel context.CancelFunc

	// Context for the current connection's pumps (read/write/ping)
	// Gets cancelled and recreated on reconnect.
	currentConnPumpCtx    context.Context
	currentConnPumpCancel context.CancelFunc
	currentConnPumpWg     sync.WaitGroup

	pendingRequestsMu sync.Mutex
	pendingRequests   map[string]chan *ergosockets.Envelope

	subscriptionHandlersMu sync.RWMutex
	subscriptionHandlers   map[string]*ergosockets.HandlerWrapper

	requestHandlersMu sync.RWMutex // For server-initiated requests
	requestHandlers   map[string]*ergosockets.HandlerWrapper

	isClosed bool // True if Close() has been called, preventing further operations/reconnects
	closedMu sync.Mutex

	reconnectingMu sync.Mutex
	isReconnecting bool // True if a reconnect loop is currently active
}

// Option configures the Client.
type Option func(*Client)

// WithLogger sets a custom logging implementation.
func WithLogger(logger *slog.Logger) Option {
	return func(c *Client) {
		if logger != nil {
			c.config.logger = logger
		}
	}
}

// WithDialOptions sets custom websocket.DialOptions.
func WithDialOptions(opts *websocket.DialOptions) Option {
	return func(c *Client) {
		c.config.dialOptions = opts
	}
}

// WithDefaultRequestTimeout sets the default timeout for cli.Request operations.
func WithDefaultRequestTimeout(timeout time.Duration) Option {
	return func(c *Client) {
		if timeout > 0 {
			c.config.defaultRequestTimeout = timeout
		}
	}
}

// WithClientPingInterval sets the client-initiated ping interval.
// interval < 0 or interval == 0: Disables client pings.
// interval > 0: Uses the specified interval.
func WithClientPingInterval(interval time.Duration) Option {
	return func(c *Client) {
		c.config.pingInterval = interval // Logic applied in New()
	}
}

// WithAutoReconnect enables automatic reconnection.
// maxAttempts = 0 means infinite attempts if autoReconnect is true.
func WithAutoReconnect(maxAttempts int, minDelay, maxDelay time.Duration) Option {
	return func(c *Client) {
		c.config.autoReconnect = true
		c.config.reconnectAttempts = maxAttempts
		if minDelay > 0 {
			c.config.reconnectDelayMin = minDelay
		}
		if maxDelay > 0 && maxDelay >= minDelay {
			c.config.reconnectDelayMax = maxDelay
		} else if maxDelay < minDelay {
			c.config.reconnectDelayMax = minDelay // Ensure max is not less than min
		}
	}
}

// WithClientName sets a custom name for the client.
func WithClientName(name string) Option {
	return func(c *Client) {
		c.config.clientName = name
	}
}

// WithClientType sets the client type.
func WithClientType(clientType string) Option {
	return func(c *Client) {
		c.config.clientType = clientType
	}
}

// WithClientURL sets the client URL.
func WithClientURL(url string) Option {
	return func(c *Client) {
		c.config.clientURL = url
	}
}

// WithContext sets a parent context for the client. When the parent context is cancelled,
// the client will shut down all operations. This allows integrating the client's lifetime
// with application-level context management.
// WithContext sets a custom context for the client, allowing for external
// lifecycle management and cancellation control. The provided context will be
// used as the parent context for all client operations.
//
// Example:
//
//	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute)
//	defer cancel()
//	client, err := Connect("ws://localhost:8080/ws", WithContext(ctx))
func WithContext(ctx context.Context) Option {
	return func(c *Client) {
		c.clientCtx, c.clientCancel = context.WithCancel(ctx)
	}
}

// WithWriteTimeout sets the write timeout for sending messages to the server.
func WithWriteTimeout(timeout time.Duration) Option {
	return func(c *Client) {
		if timeout > 0 {
			c.config.writeTimeout = timeout
		}
	}
}

// WithReadTimeout sets the read timeout for responses from the server.
func WithReadTimeout(timeout time.Duration) Option {
	return func(c *Client) {
		if timeout > 0 {
			c.config.readTimeout = timeout
		}
	}
}

// Connect establishes a WebSocket connection.
func Connect(urlStr string, opts ...Option) (*Client, error) {
	clientCtx, clientCancel := context.WithCancel(context.Background())
	cli := &Client{
		config: clientConfig{
			logger:                slog.Default(),
			defaultRequestTimeout: defaultClientReqTimeout,
			writeTimeout:          defaultWriteClientTimeout,
			readTimeout:           defaultReadClientTimeout,
			pingInterval:          libraryDefaultClientPingInterval, // Disabled by default
			reconnectDelayMin:     defaultReconnectDelayMin,
			reconnectDelayMax:     defaultReconnectDelayMax,
		},
		urlStr:               urlStr,
		id:                   ergosockets.GenerateID(),
		clientCtx:            clientCtx,
		clientCancel:         clientCancel,
		send:                 make(chan *ergosockets.Envelope, defaultClientSendBuffer),
		pendingRequests:      make(map[string]chan *ergosockets.Envelope),
		subscriptionHandlers: make(map[string]*ergosockets.HandlerWrapper),
		requestHandlers:      make(map[string]*ergosockets.HandlerWrapper),
	}

	for _, opt := range opts {
		opt(cli)
	}

	// Finalize client ping interval
	if cli.config.pingInterval < 0 { // User passed negative, means disable
		cli.config.pingInterval = 0
	}
	// If user passed 0, it uses libraryDefaultClientPingInterval (which is 0)
	// If user passed >0, it's already set.

	if cli.config.dialOptions == nil {
		cli.config.dialOptions = &websocket.DialOptions{HTTPClient: http.DefaultClient}
	}

	err := cli.establishConnection(cli.clientCtx) // Initial connection attempt
	if err != nil {
		cli.config.logger.Info(fmt.Sprintf("Client %s: Initial connection failed: %v", cli.id, err))
		if !cli.config.autoReconnect {
			cli.Close() // Clean up if not reconnecting
			return nil, fmt.Errorf("client initial connection failed and auto-reconnect disabled: %w", err)
		}
		// Auto-reconnect is enabled, start the loop.
		// establishConnection already handles logging this.
		go cli.reconnectLoop()
		// Return the client instance even if initial connect fails but reconnect is on.
		// The user can then try operations which will block/fail until connected.
	}

	return cli, nil // Return client instance, it might be in a reconnecting state
}

func (c *Client) establishConnection(ctx context.Context) error {
	c.closedMu.Lock()
	if c.isClosed {
		c.closedMu.Unlock()
		return errors.New("client is permanently closed, cannot establish connection")
	}
	c.closedMu.Unlock()

	c.connMu.Lock() // Lock to modify c.conn and pump contexts
	// If there are old pumps running for a previous connection, cancel them.
	if c.currentConnPumpCancel != nil {
		c.currentConnPumpCancel()
		c.connMu.Unlock()          // Unlock before waiting to avoid deadlock if pumps try to acquire connMu
		c.currentConnPumpWg.Wait() // Wait for old pumps to fully stop
		c.connMu.Lock()            // Re-lock
	}

	if c.conn != nil {
		c.conn.Close(websocket.StatusAbnormalClosure, "stale connection being replaced")
		c.conn = nil
	}
	c.connMu.Unlock() // Unlock before Dial, as Dial is blocking

	// Add client ID to the URL as a query parameter
	urlWithID := c.urlStr
	if !strings.Contains(urlWithID, "?") {
		urlWithID += "?"
	} else {
		urlWithID += "&"
	}
	urlWithID += "client_id=" + c.id

	dialCtx, dialCancel := context.WithTimeout(ctx, c.config.defaultRequestTimeout) // Use request timeout for dial
	conn, httpResp, err := websocket.Dial(dialCtx, urlWithID, c.config.dialOptions)
	dialCancel()

	if err != nil {
		errMsg := fmt.Sprintf("dial to %s failed: %v", urlWithID, err)
		if httpResp != nil {
			errMsg = fmt.Sprintf("%s (status: %s)", errMsg, httpResp.Status)
		}
		return errors.New(errMsg)
	}

	c.connMu.Lock()
	c.conn = conn
	// Create new context and WaitGroup for the pumps of this new connection
	c.currentConnPumpCtx, c.currentConnPumpCancel = context.WithCancel(c.clientCtx)
	c.currentConnPumpWg = sync.WaitGroup{} // Reset WaitGroup

	c.currentConnPumpWg.Add(2) // For readPump and writePump
	go c.readPump()
	go c.writePump()

	if c.config.pingInterval > 0 {
		c.currentConnPumpWg.Add(1)
		go c.pingLoop()
	}
	c.connMu.Unlock()

	c.config.logger.Info(fmt.Sprintf("Client %s: Successfully connected to %s", c.id, c.urlStr))

	// Send client registration information
	err = c.sendRegistration()
	if err != nil {
		c.config.logger.Info(fmt.Sprintf("Client %s: Failed to send registration: %v", c.id, err))
		// Close the connection and return error
		c.connMu.Lock()
		if c.conn != nil {
			c.conn.Close(websocket.StatusInternalError, "registration failed")
			c.conn = nil
		}
		c.connMu.Unlock()
		return fmt.Errorf("failed to register client: %w", err)
	}

	// Re-send subscription requests upon successful (re)connection
	c.resubscribeAll()
	return nil
}

// sendRegistration sends client registration information to the server
func (c *Client) sendRegistration() error {
	// Create registration payload
	clientURL := c.config.clientURL
	if clientURL == "" {
		clientURL = ""
	}

	clientType := c.config.clientType
	if clientType == "" {
		clientType = "generic"
	}

	// If client name is not set, generate a default one
	clientName := c.config.clientName
	if clientName == "" {
		clientName = "client-" + c.id[:8]
	}

	// Create registration request
	registration := shared_types.ClientRegistration{
		ClientID:   c.id,
		ClientName: clientName,
		ClientType: clientType,
		ClientURL:  clientURL,
	}

	// Send registration request and wait for response
	response, err := GenericRequest[shared_types.ClientRegistrationResponse](c, context.Background(), shared_types.TopicClientRegister, registration)
	if err != nil {
		return fmt.Errorf("registration request failed: %w", err)
	}

	// Update client ID with server-assigned ID
	if response != nil && response.ServerAssignedID != "" {
		c.config.logger.Info(fmt.Sprintf("Client %s: Server assigned new ID: %s", c.id, response.ServerAssignedID))
		c.id = response.ServerAssignedID
	}

	return nil
}

func (c *Client) resubscribeAll() {
	c.subscriptionHandlersMu.RLock()
	defer c.subscriptionHandlersMu.RUnlock()
	if len(c.subscriptionHandlers) > 0 {
		c.config.logger.Info(fmt.Sprintf("Client %s: Re-subscribing to %d topics...", c.id, len(c.subscriptionHandlers)))
		for topic := range c.subscriptionHandlers {
			// Fire-and-forget re-subscribe. Errors logged internally by sendSubscribeRequest.
			// A more robust system might queue these and confirm acks.
			go func(t string) { // Send in goroutine to not block establishConnection
				if err := c.sendSubscribeRequest(t); err != nil {
					c.config.logger.Info(fmt.Sprintf("Client %s: Error re-subscribing to topic '%s': %v", c.id, t, err))
				}
			}(topic)
		}
	}
}

func (c *Client) reconnectLoop() {
	c.reconnectingMu.Lock()
	if c.isReconnecting {
		c.reconnectingMu.Unlock()
		return // Another reconnect loop is already active
	}
	c.isReconnecting = true
	c.reconnectingMu.Unlock()

	defer func() {
		c.reconnectingMu.Lock()
		c.isReconnecting = false
		c.reconnectingMu.Unlock()
		c.config.logger.Info(fmt.Sprintf("Client %s: Exiting reconnect loop.", c.id))
	}()

	c.config.logger.Info(fmt.Sprintf("Client %s: Starting reconnect loop (max_attempts: %d, delay_min: %v, delay_max: %v)",
		c.id, c.config.reconnectAttempts, c.config.reconnectDelayMin, c.config.reconnectDelayMax))

	attempts := 0
	currentDelay := c.config.reconnectDelayMin

	for {
		c.closedMu.Lock()
		if c.isClosed { // Check if client was permanently closed
			c.closedMu.Unlock()
			return
		}
		c.closedMu.Unlock()

		select {
		case <-c.clientCtx.Done(): // Client is being closed permanently
			return
		default:
		}

		if c.config.reconnectAttempts > 0 && attempts >= c.config.reconnectAttempts {
			c.config.logger.Info(fmt.Sprintf("Client %s: Max reconnect attempts (%d) reached. Stopping.", c.id, c.config.reconnectAttempts))
			c.Close() // Permanently close if max attempts reached
			return
		}

		// Calculate delay with jitter
		var sleepDuration time.Duration
		if currentDelay > 0 {
			// Jitter: random percentage (e.g., 0-25%) of currentDelay
			// Add jitter to spread out retries from multiple clients
			jitterRange := int(currentDelay / 4)
			if jitterRange <= 0 {
				jitterRange = 1
			} // Ensure some jitter if delay is small
			jitter := time.Duration(rand.Intn(jitterRange))
			sleepDuration = currentDelay + jitter
		} else {
			sleepDuration = c.config.reconnectDelayMin // Should not happen if currentDelay starts at min
		}

		c.config.logger.Info(fmt.Sprintf("Client %s: Waiting %v before reconnect attempt %d...", c.id, sleepDuration, attempts+1))
		time.Sleep(sleepDuration)

		c.config.logger.Info(fmt.Sprintf("Client %s: Attempting to reconnect (attempt %d)...", c.id, attempts+1))
		err := c.establishConnection(c.clientCtx)
		if err == nil {
			c.config.logger.Info(fmt.Sprintf("Client %s: Successfully reconnected.", c.id))
			return // Exit reconnect loop on success
		}

		c.config.logger.Info(fmt.Sprintf("Client %s: Reconnect attempt %d failed: %v", c.id, attempts+1, err))
		attempts++
		currentDelay *= 2 // Exponential backoff
		if currentDelay > c.config.reconnectDelayMax {
			currentDelay = c.config.reconnectDelayMax
		}
		if currentDelay < c.config.reconnectDelayMin { // Should not happen
			currentDelay = c.config.reconnectDelayMin
		}
	}
}

func (c *Client) getConn() *websocket.Conn {
	c.connMu.RLock()
	defer c.connMu.RUnlock()
	return c.conn
}

func (c *Client) readPump() {
	defer func() {
		c.config.logger.Info(fmt.Sprintf("Client %s: readPump stopping for connection.", c.id))
		// Signal other pumps (write, ping) for THIS connection to stop.
		if c.currentConnPumpCancel != nil {
			c.currentConnPumpCancel()
		}

		c.connMu.Lock()
		if c.conn != nil {
			// It's important that Close is called on the specific connection instance
			// this readPump was associated with, not potentially a new one from a concurrent reconnect.
			// However, getConn() inside the loop should always refer to the conn this pump started with.
			c.conn.Close(websocket.StatusAbnormalClosure, "read pump terminated for connection")
			c.conn = nil // Indicate no active connection
		}
		c.connMu.Unlock()

		c.currentConnPumpWg.Done() // Signal completion of this pump

		// If auto-reconnect is enabled and client is not permanently closed, trigger reconnect.
		c.closedMu.Lock()
		isPermanentlyClosed := c.isClosed
		c.closedMu.Unlock()

		if c.config.autoReconnect && !isPermanentlyClosed {
			c.reconnectingMu.Lock()
			// Only start a new reconnectLoop if one isn't already running
			if !c.isReconnecting {
				c.reconnectingMu.Unlock() // Unlock before starting goroutine
				go c.reconnectLoop()
			} else {
				c.reconnectingMu.Unlock()
				c.config.logger.Info(fmt.Sprintf("Client %s: readPump detected disconnect, but reconnect loop already active.", c.id))
			}
		}
	}()

	cfg := c.config
	readDeadlineDuration := cfg.readTimeout
	// If server pings are expected, client read deadline should accommodate them.
	// If client pings are enabled, this logic also applies.
	if cfg.pingInterval > 0 { // Client pings enabled
		readDeadlineDuration = cfg.pingInterval * 2
		if readDeadlineDuration < cfg.readTimeout {
			readDeadlineDuration = cfg.readTimeout
		}
	} else { // Rely on server pings, use readTimeout (should be > server ping interval)
		// This assumes server pings are more frequent than readTimeout.
	}

	currentLocalConn := c.getConn() // Get the connection this pump is for
	if currentLocalConn == nil {
		c.config.logger.Info(fmt.Sprintf("Client %s: readPump started with nil connection.", c.id))
		return
	}

	if readDeadlineDuration > 0 {
		currentLocalConn.SetReadLimit(1024 * 1024) // 1MB
		// currentLocalConn.SetReadDeadline(ergosockets.TimeNow().Add(readDeadlineDuration))
		// currentLocalConn.P(func(string) error {
		// 	// c.config.logger.Info(fmt.Sprintf("Client %s: Pong received", c.id)
		// 	// Refresh deadline only on the connection this handler is for.
		// 	activeConn := c.getConn()
		// 	if activeConn != nil && readDeadlineDuration > 0 {
		// 		_ = activeConn.SetReadDeadline(ergosockets.TimeNow().Add(readDeadlineDuration))
		// 	}
		// 	return nil
		// })
	}

	for {
		// Use currentConnPumpCtx for reads, as it's tied to this specific connection's lifecycle.
		select {
		case <-c.currentConnPumpCtx.Done():
			c.config.logger.Info(fmt.Sprintf("Client %s: readPump stopping due to current connection pump context.", c.id))
			return
		default:
		}

		var env ergosockets.Envelope
		err := wsjson.Read(c.currentConnPumpCtx, currentLocalConn, &env)
		if err != nil {
			status := websocket.CloseStatus(err)
			select {
			case <-c.currentConnPumpCtx.Done():
				c.config.logger.Info(fmt.Sprintf("Client %s: readPump gracefully closing after context cancellation (err: %v)", c.id, err))
			case <-c.clientCtx.Done():
				c.config.logger.Info(fmt.Sprintf("Client %s: readPump closing due to permanent client shutdown (err: %v)", c.id, err))
			default: // Actual read error
				if status != websocket.StatusNormalClosure && status != websocket.StatusGoingAway && !errors.Is(err, context.Canceled) {
					c.config.logger.Info(fmt.Sprintf("Client %s: read error in readPump: %v (status: %d)", c.id, err, status))
				} else {
					c.config.logger.Info(fmt.Sprintf("Client %s: readPump normal websocket closure: %v (status: %d)", c.id, err, status))
				}
			}
			return // Exit loop, defer will handle cleanup/reconnect
		}

		// if readDeadlineDuration > 0 {
		// DOSNT WORK with WS CONN
		// 	currentLocalConn.SetReadDeadline(ergosockets.TimeNow().Add(readDeadlineDuration))
		// }

		switch env.Type {
		case ergosockets.TypeResponse, ergosockets.TypeError:
			c.pendingRequestsMu.Lock()
			if ch, ok := c.pendingRequests[env.ID]; ok {
				select {
				case ch <- &env:
				default:
					c.config.logger.Info(fmt.Sprintf("Client %s: Response channel for ID %s not ready or already processed", c.id, env.ID))
				}
			} else {
				c.config.logger.Info(fmt.Sprintf("Client %s: Received unsolicited server-targeted response/error with ID %s", c.id, env.ID))
			}
			c.pendingRequestsMu.Unlock()
		case ergosockets.TypePublish:
			c.subscriptionHandlersMu.RLock()
			hw, ok := c.subscriptionHandlers[env.Topic]
			c.subscriptionHandlersMu.RUnlock()
			if ok {
				go c.invokeSubscriptionHandler(hw, &env)
			} else {
				// c.config.logger.Info(fmt.Sprintf("Client %s: No subscription handler for publish topic '%s'", c.id, env.Topic)
			}
		case ergosockets.TypeRequest: // Server-initiated request
			c.requestHandlersMu.RLock()
			hw, ok := c.requestHandlers[env.Topic]
			c.requestHandlersMu.RUnlock()
			if ok {
				go c.invokeClientRequestHandler(hw, &env)
			} else {
				c.config.logger.Info(fmt.Sprintf("Client %s: No handler for server request on topic '%s'", c.id, env.Topic))
				errEnv, _ := ergosockets.NewEnvelope(env.ID, ergosockets.TypeError, env.Topic, nil,
					&ergosockets.ErrorPayload{Code: http.StatusNotFound, Message: "Client has no handler for topic: " + env.Topic})
				c.trySend(errEnv)
			}
		case ergosockets.TypeSubscriptionAck:
			c.config.logger.Info(fmt.Sprintf("Client %s: Received subscription ack for topic '%s' (ID: %s)", c.id, env.Topic, env.ID))
		default:
			c.config.logger.Info(fmt.Sprintf("Client %s: Received unknown envelope type: '%s'", c.id, env.Type))
		}
	}
}

func (c *Client) invokeSubscriptionHandler(hw *ergosockets.HandlerWrapper, env *ergosockets.Envelope) {
	arg, err := ergosockets.DecodeAndPrepareArg(env.Payload, hw.MsgType)
	if err != nil {
		c.config.logger.Info(fmt.Sprintf("Client %s: Failed to decode publish payload for topic '%s': %v", c.id, env.Topic, err))
		return
	}

	results := hw.HandlerFunc.Call([]reflect.Value{arg})
	if errVal, ok := results[0].Interface().(error); ok && errVal != nil {
		c.config.logger.Info(fmt.Sprintf("Client %s: Subscription handler for topic '%s' returned error: %v", c.id, env.Topic, errVal))
	}
}

func (c *Client) invokeClientRequestHandler(hw *ergosockets.HandlerWrapper, reqEnv *ergosockets.Envelope) {
	reqArg, err := ergosockets.DecodeAndPrepareArg(reqEnv.Payload, hw.ReqType)
	if err != nil {
		c.config.logger.Info(fmt.Sprintf("Client %s: Failed to decode server request payload for topic '%s': %v", c.id, reqEnv.Topic, err))
		errResp, _ := ergosockets.NewEnvelope(reqEnv.ID, ergosockets.TypeError, reqEnv.Topic, nil, &ergosockets.ErrorPayload{Code: http.StatusBadRequest, Message: "Invalid request payload from server: " + err.Error()})
		c.trySend(errResp)
		return
	}

	inputs := []reflect.Value{reqArg}
	results := hw.HandlerFunc.Call(inputs)

	var errResult error
	if errVal, ok := results[len(results)-1].Interface().(error); ok {
		errResult = errVal
	}

	if errResult != nil {
		c.config.logger.Info(fmt.Sprintf("Client %s: HandleServerRequest handler for server topic '%s' returned error: %v", c.id, reqEnv.Topic, errResult))
		errResp, _ := ergosockets.NewEnvelope(reqEnv.ID, ergosockets.TypeError, reqEnv.Topic, nil, &ergosockets.ErrorPayload{Code: http.StatusInternalServerError, Message: errResult.Error()})
		c.trySend(errResp)
		return
	}

	if hw.RespType != nil { // Handler returns a response payload
		respPayload := results[0].Interface()
		respEnv, err := ergosockets.NewEnvelope(reqEnv.ID, ergosockets.TypeResponse, reqEnv.Topic, respPayload, nil)
		if err != nil {
			c.config.logger.Info(fmt.Sprintf("Client %s: Failed to create response envelope for server request on topic '%s': %v", c.id, reqEnv.Topic, err))
			errResp, _ := ergosockets.NewEnvelope(reqEnv.ID, ergosockets.TypeError, reqEnv.Topic, nil, &ergosockets.ErrorPayload{Code: http.StatusInternalServerError, Message: "Client failed to create response envelope"})
			c.trySend(errResp)
			return
		}
		c.trySend(respEnv)
	} else if reqEnv.ID != "" { // No response payload, but request had an ID, send simple ack
		ackEnv, _ := ergosockets.NewEnvelope(reqEnv.ID, ergosockets.TypeResponse, reqEnv.Topic, nil, nil)
		c.trySend(ackEnv)
	}
}

func (c *Client) trySend(env *ergosockets.Envelope) {
	select {
	case c.send <- env:
	case <-c.currentConnPumpCtx.Done(): // Use pumpCtx as this is response for current connection
		c.config.logger.Info(fmt.Sprintf("Client %s: Current connection pump context done, cannot send envelope type %s on topic %s", c.id, env.Type, env.Topic))
	case <-c.clientCtx.Done(): // Or main client context if permanently closing
		c.config.logger.Info(fmt.Sprintf("Client %s: Main client context done, cannot send envelope type %s on topic %s", c.id, env.Type, env.Topic))
	default:
		c.config.logger.Info(fmt.Sprintf("Client %s: Send channel full when trying to send envelope type %s on topic %s. Message dropped.", c.id, env.Type, env.Topic))
	}
}

func (c *Client) writePump() {
	defer func() {
		c.config.logger.Info(fmt.Sprintf("Client %s: writePump stopping for connection.", c.id))
		// If readPump initiated shutdown via currentConnPumpCancel, this is fine.
		// If writePump fails first, it should also signal currentConnPumpCancel.
		c.currentConnPumpWg.Done()
	}()

	for {
		select {
		case message, ok := <-c.send:
			if !ok { // send channel closed, likely by Client.Close()
				c.config.logger.Info(fmt.Sprintf("Client %s: Send channel closed, writePump exiting.", c.id))
				// Ensure connection is closed if not already
				if conn := c.getConn(); conn != nil {
					conn.Close(websocket.StatusNormalClosure, "client send channel closed by master")
				}
				return
			}
			conn := c.getConn()
			if conn == nil {
				c.config.logger.Info(fmt.Sprintf("Client %s: writePump: no active connection, message dropped: Topic=%s, Type=%s", c.id, message.Topic, message.Type))
				// If auto-reconnect is on, message might be lost. A more robust system might queue.
				continue
			}
			// Use currentConnPumpCtx for the write operation's timeout context
			writeOpCtx, writeOpCancel := context.WithTimeout(c.currentConnPumpCtx, c.config.writeTimeout)
			err := wsjson.Write(writeOpCtx, conn, message)
			writeOpCancel()

			if err != nil {
				c.config.logger.Info(fmt.Sprintf("Client %s: write error in writePump: %v. Connection may be stale.", c.id, err))
				// A write error often means the connection is bad.
				// Signal other pumps for this specific connection to stop.
				if c.currentConnPumpCancel != nil {
					c.currentConnPumpCancel() // This will cause readPump and pingLoop for this conn to exit.
				}
				// readPump's defer will handle potential reconnect.
				return // Exit writePump for this connection.
			}
		case <-c.currentConnPumpCtx.Done(): // Current connection's pumps are shutting down
			c.config.logger.Info(fmt.Sprintf("Client %s: writePump stopping due to current connection pump context.", c.id))
			// Connection should be closed by the goroutine that cancelled currentConnPumpCtx (e.g. readPump)
			return
		case <-c.clientCtx.Done(): // Client is being closed permanently
			c.config.logger.Info(fmt.Sprintf("Client %s: writePump stopping due to permanent client shutdown.", c.id))
			return
		}
	}
}

func (c *Client) pingLoop() {
	defer func() {
		c.config.logger.Info(fmt.Sprintf("Client %s: pingLoop stopping for connection.", c.id))
		c.currentConnPumpWg.Done()
	}()

	if c.config.pingInterval <= 0 { // Guard: only run if interval is positive
		return
	}
	ticker := time.NewTicker(c.config.pingInterval)
	defer ticker.Stop()
	c.config.logger.Info(fmt.Sprintf("Client %s: PingLoop started with interval %v", c.id, c.config.pingInterval))

	for {
		select {
		case <-ticker.C:
			conn := c.getConn()
			if conn == nil {
				// c.config.logger.Info(fmt.Sprintf("Client %s: pingLoop: no active connection, skipping ping.", c.id)
				continue // Wait for reconnect
			}
			// Use currentConnPumpCtx for the ping operation
			pingOpCtx, pingOpCancel := context.WithTimeout(c.currentConnPumpCtx, c.config.pingInterval/2) // Shorter timeout for ping
			err := conn.Ping(pingOpCtx)
			pingOpCancel()
			if err != nil {
				c.config.logger.Info(fmt.Sprintf("Client %s: Ping failed: %v. Connection might be stale.", c.id, err))
				// Signal other pumps for this specific connection to stop.
				if c.currentConnPumpCancel != nil {
					c.currentConnPumpCancel()
				}
				return // Exit ping loop for this connection. readPump's defer will handle reconnect.
			}
			// c.config.logger.Info(fmt.Sprintf("Client %s: Ping sent.", c.id)
		case <-c.currentConnPumpCtx.Done():
			return // Current connection's pumps are shutting down
		case <-c.clientCtx.Done(): // Client is being closed permanently
			return
		}
	}
}

// SendServerRequest sends a request to the server and waits for a response of type T.
// The first optional payload argument (reqData) is the request data.
// If no reqData is provided, a null payload is sent.
func (c *Client) SendServerRequest(ctx context.Context, topic string, reqData ...interface{}) (*json.RawMessage, *ergosockets.ErrorPayload, error) {
	c.closedMu.Lock()
	if c.isClosed {
		c.closedMu.Unlock()
		return nil, nil, errors.New("client is closed")
	}
	c.closedMu.Unlock()

	var requestPayload interface{}
	if len(reqData) > 0 {
		requestPayload = reqData[0]
	}
	// If len(reqData) == 0, requestPayload remains nil.
	// ergosockets.NewEnvelope handles nil payloadData by setting Envelope.Payload to nil,
	// which json.Marshal then serializes as JSON `null`.

	correlationID := ergosockets.GenerateID()
	// Envelope.Payload will be `null` if requestPayload is nil.
	reqEnv, err := ergosockets.NewEnvelope(correlationID, ergosockets.TypeRequest, topic, requestPayload, nil)
	if err != nil {
		return nil, nil, fmt.Errorf("client: failed to create request envelope for topic '%s': %w", topic, err)
	}

	respChan := make(chan *ergosockets.Envelope, 1)
	c.pendingRequestsMu.Lock()
	c.pendingRequests[correlationID] = respChan
	c.pendingRequestsMu.Unlock()

	defer func() {
		c.pendingRequestsMu.Lock()
		delete(c.pendingRequests, correlationID)
		c.pendingRequestsMu.Unlock()
		// Do not close respChan here, as the receiver might still be selecting on it if a timeout occurred elsewhere.
		// It will be garbage collected.
	}()

	// Determine effective timeout for the entire operation
	effectiveTimeout := c.config.defaultRequestTimeout
	if deadline, ok := ctx.Deadline(); ok {
		if timeout := time.Until(deadline); timeout < effectiveTimeout {
			effectiveTimeout = timeout
		}
	}
	// Create a new context for this specific request operation, derived from user's ctx
	requestOpCtx, requestOpCancel := context.WithTimeout(ctx, effectiveTimeout)
	defer requestOpCancel()

	// Send the request envelope
	select {
	case c.send <- reqEnv:
		c.config.logger.Info(fmt.Sprintf("Client %s: Sent request (ID: %s) on topic '%s'", c.id, correlationID, topic))
	case <-requestOpCtx.Done(): // Timeout or cancellation before send
		return nil, nil, fmt.Errorf("client: context done before sending request %s: %w", correlationID, requestOpCtx.Err())
	case <-c.clientCtx.Done(): // Client permanently closing
		return nil, nil, fmt.Errorf("client: client permanently closing before sending request %s: %w", correlationID, c.clientCtx.Err())
	}

	// Wait for the response envelope using the requestOpCtx
	select {
	case respEnv, ok := <-respChan:
		if !ok { // Channel was closed unexpectedly (should not happen with current defer logic)
			return nil, nil, fmt.Errorf("client: response channel closed for request ID %s (connection issue?)", correlationID)
		}
		if respEnv.Error != nil {
			return nil, respEnv.Error, fmt.Errorf("client: server error for request ID %s (code %d): %s", correlationID, respEnv.Error.Code, respEnv.Error.Message)
		}
		// Return raw payload and nil error for successful response
		return &respEnv.Payload, nil, nil // Note: returning pointer to allow distinguishing nil payload from no payload
	case <-requestOpCtx.Done(): // Timeout or cancellation while waiting for response
		return nil, nil, fmt.Errorf("client: request ID %s timed out or context cancelled after %v: %w", correlationID, effectiveTimeout, requestOpCtx.Err())
	case <-c.clientCtx.Done(): // Client permanently closing
		return nil, nil, fmt.Errorf("client: client permanently closing while waiting for response %s: %w", correlationID, c.clientCtx.Err())
	}
}

// Publish sends a fire-and-forget message to the server.
func (c *Client) Publish(topic string, payloadData interface{}) error {
	c.closedMu.Lock()
	if c.isClosed {
		c.closedMu.Unlock()
		return errors.New("client is closed")
	}
	c.closedMu.Unlock()

	// Envelope.Payload will be `null` if payloadData is nil.
	env, err := ergosockets.NewEnvelope("", ergosockets.TypePublish, topic, payloadData, nil)
	if err != nil {
		return fmt.Errorf("client: failed to create publish envelope for topic '%s': %w", topic, err)
	}

	// Use a short, non-blocking attempt to send, or timeout quickly.
	// Publish is fire-and-forget, so we don't want it to block the caller for long.
	select {
	case c.send <- env:
		// c.config.logger.Info(fmt.Sprintf("Client %s: Published message on topic '%s'", c.id, topic)
		return nil
	case <-c.clientCtx.Done():
		return fmt.Errorf("client: client permanently closing, cannot publish: %w", c.clientCtx.Err())
	case <-time.After(c.config.writeTimeout / 2): // Use a fraction of write timeout
		return fmt.Errorf("client: publish to topic '%s' timed out (send channel likely full or connection stalled)", topic)
	}
}

// Subscribe registers a handler for messages published by the server on a given topic.
// handlerFunc must be of type: func(MsgStruct) error or func(*MsgStruct) error
func (c *Client) Subscribe(topic string, handlerFunc interface{}) (unsubscribeFunc func(), err error) {
	c.closedMu.Lock()
	if c.isClosed {
		c.closedMu.Unlock()
		return nil, errors.New("client is closed")
	}
	c.closedMu.Unlock()

	hw, err := ergosockets.NewHandlerWrapper(handlerFunc)
	if err != nil {
		return nil, fmt.Errorf("client Subscribe topic '%s': %w", topic, err)
	}
	// Validate client subscribe signature (1 in, 1 out error)
	if hw.HandlerFunc.Type().NumIn() != 1 || !(hw.HandlerFunc.Type().NumOut() == 1 && hw.HandlerFunc.Type().Out(0) == ergosockets.ErrType) {
		return nil, fmt.Errorf("client Subscribe topic '%s': handler must be func(MessageType) error, got %s", topic, hw.HandlerFunc.Type().String())
	}

	c.subscriptionHandlersMu.Lock()
	if _, exists := c.subscriptionHandlers[topic]; exists {
		c.subscriptionHandlersMu.Unlock()
		return nil, fmt.Errorf("client: handler already subscribed to topic '%s'", topic)
	}
	c.subscriptionHandlers[topic] = hw
	c.subscriptionHandlersMu.Unlock()

	if err := c.sendSubscribeRequest(topic); err != nil {
		c.subscriptionHandlersMu.Lock()
		delete(c.subscriptionHandlers, topic) // Rollback local registration
		c.subscriptionHandlersMu.Unlock()
		return nil, fmt.Errorf("client: failed to send subscribe request for topic '%s': %w", topic, err)
	}
	c.config.logger.Info(fmt.Sprintf("Client %s: Subscribed to topic '%s'", c.id, topic))

	unsubscribe := func() {
		c.subscriptionHandlersMu.Lock()
		delete(c.subscriptionHandlers, topic)
		c.subscriptionHandlersMu.Unlock()
		_ = c.sendUnsubscribeRequest(topic) // Fire and forget
		c.config.logger.Info(fmt.Sprintf("Client %s: Unsubscribed from topic '%s'", c.id, topic))
	}
	return unsubscribe, nil
}

func (c *Client) sendSubscribeRequest(topic string) error {
	// Use a short timeout for control messages.
	ctx, cancel := context.WithTimeout(c.clientCtx, c.config.writeTimeout)
	defer cancel()

	// ID for subscribe request can be used to correlate server's ack if needed
	subEnv, err := ergosockets.NewEnvelope(ergosockets.GenerateID(), ergosockets.TypeSubscribeRequest, topic, nil, nil)
	if err != nil {
		return err // Should be rare
	}
	select {
	case c.send <- subEnv:
		return nil
	case <-ctx.Done():
		return fmt.Errorf("sending subscribe request for topic '%s': %w", topic, ctx.Err())
	case <-c.clientCtx.Done(): // Check permanent close too
		return fmt.Errorf("client permanently closing, cannot send subscribe for topic '%s': %w", topic, c.clientCtx.Err())
	}
}
func (c *Client) sendUnsubscribeRequest(topic string) error {
	ctx, cancel := context.WithTimeout(c.clientCtx, c.config.writeTimeout)
	defer cancel()
	unsubEnv, _ := ergosockets.NewEnvelope(ergosockets.GenerateID(), ergosockets.TypeUnsubscribeRequest, topic, nil, nil)
	select {
	case c.send <- unsubEnv:
		return nil
	case <-ctx.Done():
		return fmt.Errorf("sending unsubscribe request for topic '%s': %w", topic, ctx.Err())
	case <-c.clientCtx.Done():
		return fmt.Errorf("client permanently closing, cannot send unsubscribe for topic '%s': %w", topic, c.clientCtx.Err())
	}
}

// HandleServerRequest registers a handler for requests initiated by the server on a given topic.
// handlerFunc must be of type: func(ReqStruct) (RespStruct, error) or func(ReqStruct) error
// or func(*ReqStruct) (*RespStruct, error) etc.
func (c *Client) HandleServerRequest(topic string, handlerFunc interface{}) error {
	c.closedMu.Lock()
	if c.isClosed {
		c.closedMu.Unlock()
		return errors.New("client is closed")
	}
	c.closedMu.Unlock()

	hw, err := ergosockets.NewHandlerWrapper(handlerFunc)
	if err != nil {
		return fmt.Errorf("client HandleServerRequest topic '%s': %w", topic, err)
	}
	// Validate client HandleServerRequest signature (1 in, 1 or 2 out with last as error)
	if hw.HandlerFunc.Type().NumIn() != 1 {
		return fmt.Errorf("client HandleServerRequest topic '%s': handler must have 1 input argument (RequestType), got %d", topic, hw.HandlerFunc.Type().NumIn())
	}

	c.requestHandlersMu.Lock()
	defer c.requestHandlersMu.Unlock()
	if _, exists := c.requestHandlers[topic]; exists {
		return fmt.Errorf("client: handler already registered for server requests on topic '%s'", topic)
	}
	c.requestHandlers[topic] = hw
	c.config.logger.Info(fmt.Sprintf("Client %s: Registered handler for server requests on topic '%s'", c.id, topic))
	return nil
}

// ID returns the unique ID of this client instance.
func (c *Client) ID() string {
	return c.id
}

// Close gracefully closes the client connection and stops all operations.
// It cancels the main client context, signals internal goroutines to stop,
// and closes the WebSocket connection.
func (c *Client) Close() error {
	c.closedMu.Lock()
	if c.isClosed {
		c.closedMu.Unlock()
		return errors.New("client: already closed or closing")
	}
	c.isClosed = true // Mark as closed to prevent new operations/reconnects
	c.closedMu.Unlock()

	c.config.logger.Info(fmt.Sprintf("Client %s: Initiating close...", c.id))

	// 1. Cancel the main client context. This signals all derived contexts,
	// including currentConnPumpCtx (if active) and any pending operation contexts.
	if c.clientCancel != nil {
		c.clientCancel()
	}

	// 2. Close the `send` channel. This will make the writePump exit if it's ranging.
	// Do this early to prevent new messages from being queued during shutdown.
	// Ensure it's only closed once.
	// This needs to be coordinated with writePump's select.
	// For simplicity, we rely on context cancellation to stop writePump.
	// Closing send channel here might cause panic if writePump tries to send to it after check.
	// Better to let writePump exit via context and then it won't read from `send`.
	// However, if writePump is blocked on `c.send <- message`, closing `send` unblocks it.
	// This needs careful thought. A dedicated close signal for send might be better.
	// For now, rely on context cancellation.

	// 3. Wait for current connection's pumps to stop.
	// This wait should happen *after* clientCancel, as that's what signals them.
	// currentConnPumpCancel is called by readPump when it exits, or when a new conn is made.
	// If Close is called, clientCancel will propagate to currentConnPumpCtx.
	c.currentConnPumpWg.Wait() // Wait for read, write, ping loops of the *last active connection*

	// 4. Close the actual WebSocket connection if it's still open.
	// (It might have been closed by readPump already).
	c.connMu.Lock()
	if c.conn != nil {
		c.config.logger.Info(fmt.Sprintf("Client %s: Closing WebSocket connection explicitly.", c.id))
		// Use a short timeout for the close handshake.
		_, cancel := context.WithTimeout(context.Background(), 2*time.Second)
		defer cancel()
		c.conn.Close(websocket.StatusNormalClosure, "client initiated close")
		c.conn = nil
	}
	c.connMu.Unlock()

	// Now safe to close `send` channel as writePump should have exited.
	// This is to unblock any goroutines that might be stuck trying to send to `c.send`
	// if they weren't using context properly (though library methods should).
	// This needs to be idempotent or guarded.
	// For now, assume context cancellation is sufficient for writePump to exit.
	// close(c.send) // This can panic if already closed or writePump is still trying to send.

	c.config.logger.Info(fmt.Sprintf("Client %s: Close sequence complete.", c.id))
	return nil
}

// GenericRequest is the primary method for client-to-server requests.
// It handles sending the request and unmarshalling the response payload into type T.
// reqData is variadic:
// - If no reqData: sends a request with a JSON `null` payload.
// - If one reqData: uses it as the payload.
// - More than one reqData is a usage error (takes the first).
func GenericRequest[T any](cli *Client, ctx context.Context, topic string, reqData ...interface{}) (*T, error) {
	rawPayload, serverErrPayload, err := cli.SendServerRequest(ctx, topic, reqData...)
	if err != nil {
		if serverErrPayload != nil {
			return nil, fmt.Errorf("server error (code %d): %s (underlying client/network error: %w)", serverErrPayload.Code, serverErrPayload.Message, err)
		}
		return nil, err // Client-side error (e.g., timeout, context cancelled before send, network issue)
	}

	// If rawPayload is nil or points to JSON "null"
	if rawPayload == nil || string(*rawPayload) == "null" {
		var zero T
		// If T is a pointer type or an empty struct, a null payload might be acceptable.
		rt := reflect.TypeOf(zero)
		if rt == nil { // T is interface{}
			return nil, nil // Cannot determine, return nil
		}
		if rt.Kind() == reflect.Ptr || (rt.Kind() == reflect.Struct && rt.NumField() == 0) {
			// For pointer types or empty structs, a null payload results in a nil pointer or zero struct.
			return new(T), nil // Return pointer to zero value of T
		}
		return nil, fmt.Errorf("server returned successful response with null/no payload, but expected non-empty type %T", zero)
	}

	var typedResponse T
	if err := json.Unmarshal(*rawPayload, &typedResponse); err != nil {
		return nil, fmt.Errorf("failed to unmarshal response payload into %T: %w. Raw payload: %s", typedResponse, err, string(*rawPayload))
	}
	return &typedResponse, nil
}

// SendToClientRequest sends a request to another client (peer) via the broker
// and unmarshals the peer's response into the specified type T.
//
// Parameters:
//   - cli: The client instance initiating the request.
//   - ctx: Context for the operation.
//   - targetClientID: The ID of the destination client.
//   - topic: The application-specific topic the target client should handle.
//   - requestData: (Optional) The payload for the request to the target client.
//
// Returns:
//   - *T: A pointer to the unmarshalled response from the target client.
//   - error: An error if the operation failed at any stage (sending, broker error, target error, unmarshalling).
//
// Example:
//   type MyTargetRequest struct { Input string }
//   type MyTargetResponse struct { Output string }
//   resp, err := client.SendToClientRequest[MyTargetResponse](
//       myClient, ctx, "target-client-id", "target.topic", MyTargetRequest{Input: "hello"},
//   )
func SendToClientRequest[T any](cli *Client, ctx context.Context, targetClientID string, topic string, requestData ...interface{}) (*T, error) {
	cli.closedMu.Lock()
	if cli.isClosed {
		cli.closedMu.Unlock()
		return nil, errors.New("client is closed")
	}
	cli.closedMu.Unlock()

	var actualPayload interface{}
	if len(requestData) > 0 {
		actualPayload = requestData[0]
	}

	// Marshal the actual application payload destined for the target client
	actualPayloadBytes, err := json.Marshal(actualPayload)
	if err != nil {
		return nil, fmt.Errorf("client: SendToClientRequest failed to marshal actual payload for topic '%s': %w", topic, err)
	}

	// Create the ProxyRequest wrapper
	proxyReqPayload := shared_types.ProxyRequest{
		TargetID: targetClientID,
		Topic:    topic,
		Payload:  actualPayloadBytes, // json.RawMessage
	}

	// Send the ProxyRequest to the broker's system:proxy topic
	// The response from this will be the json.RawMessage from the target client
	rawPeerResponse, brokerOrTargetErrorPayload, err := cli.SendServerRequest(ctx, shared_types.TopicProxyRequest, proxyReqPayload)

	if err != nil {
		// This 'err' could be from cli.SendServerRequest (e.g., network, local timeout)
		// or an error envelope returned by the broker itself (e.g., target client not found)
		// or an error envelope from the target client.
		if brokerOrTargetErrorPayload != nil {
			return nil, fmt.Errorf("client: SendToClientRequest received error response for proxy to target '%s', topic '%s' (code %d): %s. Original error: %w", targetClientID, topic, brokerOrTargetErrorPayload.Code, brokerOrTargetErrorPayload.Message, err)
		}
		return nil, fmt.Errorf("client: SendToClientRequest failed for target '%s', topic '%s': %w", targetClientID, topic, err)
	}

	// Handle cases where the target client might return no payload (or JSON null)
	if rawPeerResponse == nil || string(*rawPeerResponse) == "null" {
		var zero T
		// If T is a pointer type or an empty struct, a null payload might be acceptable.
		rt := reflect.TypeOf(zero)
		if rt == nil { // T is interface{}
			return nil, nil // Cannot determine, return nil
		}
		if rt.Kind() == reflect.Ptr || (rt.Kind() == reflect.Struct && rt.NumField() == 0) {
			// For pointer types or empty structs, a null payload results in a nil pointer or zero struct.
			return new(T), nil // Return pointer to zero value of T
		}
		return nil, fmt.Errorf("client: SendToClientRequest to target '%s', topic '%s' returned successful response with null/no payload, but expected non-empty type %T", targetClientID, topic, zero)
	}

	// Unmarshal the target client's actual response
	var typedResponse T
	if err := json.Unmarshal(*rawPeerResponse, &typedResponse); err != nil {
		return nil, fmt.Errorf("client: SendToClientRequest failed to unmarshal response from target '%s', topic '%s' into %T: %w. Raw payload: %s", targetClientID, topic, typedResponse, err, string(*rawPeerResponse))
	}

	return &typedResponse, nil
}
```
**File: pkg/hotreload/hotreload.go**
```go
// Package hotreload provides hot reload functionality for web applications.
package hotreload

import (
	"context"
	"fmt" // Added for error wrapping
	"log/slog"
	"os"
	"sync"
	"time"

	"github.com/lightforgemedia/go-websocketmq/pkg/broker"
	"github.com/lightforgemedia/go-websocketmq/pkg/filewatcher"
)

// Constants for topics
const (
	TopicHotReload      = "system:hot_reload"
	TopicClientError    = "system:client_error"
	TopicHotReloadReady = "system:hotreload_ready"
)

// HotReload coordinates file watching and browser reloading
type HotReload struct {
	broker    *broker.Broker
	watcher   *filewatcher.FileWatcher
	logger    *slog.Logger
	clients   map[string]*clientInfo
	clientsMu sync.RWMutex
	options   Options
}

// clientInfo stores information about a connected client
type clientInfo struct {
	id       string
	errors   []clientError
	status   string
	url      string
	lastSeen time.Time
}

// clientError represents a JavaScript error from a client
type clientError struct {
	message   string
	filename  string
	lineno    int
	colno     int
	stack     string
	timestamp string
}

// New creates a new HotReload service
func New(opts ...Option) (*HotReload, error) {
	// Create with default options
	hr := &HotReload{
		clients: make(map[string]*clientInfo),
		logger:  slog.New(slog.NewTextHandler(os.Stderr, nil)),
		options: DefaultOptions(),
	}

	// Apply options
	for _, opt := range opts {
		opt(hr)
	}

	// Validate required fields
	if hr.broker == nil {
		return nil, ErrNoBroker
	}

	if hr.watcher == nil {
		return nil, ErrNoFileWatcher
	}

	return hr, nil
}

// Start starts the hot reload service
func (hr *HotReload) Start() error {
	// Set up file watcher callback
	hr.watcher.AddCallback(hr.handleFileChange)

	// Start the file watcher
	if err := hr.watcher.Start(); err != nil {
		return fmt.Errorf("failed to start file watcher: %w", err)
	}

	// Set up broker request handler for client errors.
	// This assumes the JavaScript client sends errors as a REQUEST to TopicClientError.
	err := hr.broker.HandleClientRequest(TopicClientError, func(client broker.ClientHandle, payload map[string]interface{}) error {
		hr.handleClientError(client.ID(), payload)
		// No response payload needed, successful handling of the request is enough.
		return nil
	})
	if err != nil {
		hr.logger.Error("Failed to register client error handler with broker", "error", err)
		// Depending on severity, you might want to stop or return the error
		return fmt.Errorf("failed to setup client error handler: %w", err)
	}

	// Set up broker request handler for client ready notifications
	// This assumes the JavaScript client sends a REQUEST to TopicHotReloadReady.
	err = hr.broker.HandleClientRequest(TopicHotReloadReady, func(client broker.ClientHandle, payload map[string]interface{}) error {
		hr.handleClientReady(client.ID(), payload)
		return nil // Acknowledge ready notification
	})
	if err != nil {
		hr.logger.Error("Failed to register client ready handler with broker", "error", err)
		return fmt.Errorf("failed to setup client ready handler: %w", err)
	}

	hr.logger.Info("Hot reload service started")
	return nil
}

// Stop stops the hot reload service
func (hr *HotReload) Stop() error {
	// Stop the file watcher
	if err := hr.watcher.Stop(); err != nil {
		// Log the error but continue, as we still want to indicate the service is "stopped"
		hr.logger.Error("Error stopping file watcher", "error", err)
	}

	hr.logger.Info("Hot reload service stopped")
	return nil // Or return watcher.Stop() error if it's critical
}

// handleFileChange is called when a file changes
func (hr *HotReload) handleFileChange(file string) {
	hr.logger.Info("File changed, triggering hot reload", "file", file)

	// Notify all connected clients
	hr.triggerReload()
}

// triggerReload sends a reload command to all connected clients
func (hr *HotReload) triggerReload() {
	// Get all connected clients
	var clientIDs []string
	hr.broker.IterateClients(func(client broker.ClientHandle) bool {
		// Optionally, filter clients that are "ready" for hot reload
		hr.clientsMu.RLock()
		info, exists := hr.clients[client.ID()]
		hr.clientsMu.RUnlock()
		if exists && info.status == "ready" {
			clientIDs = append(clientIDs, client.ID())
		} else if !exists { // Client connected to broker but not yet "ready" with hotreload service
			hr.logger.Debug("Client not yet ready for hot reload, skipping", "client_id", client.ID())
		}
		return true
	})

	if len(clientIDs) == 0 {
		hr.logger.Info("No ready clients to send hot reload command to.")
		return
	}

	hr.logger.Info("Sending reload command", "client_count", len(clientIDs))
	// Send reload command to each client
	// The payload for TopicHotReload can be simple, e.g., an empty struct or a map with file info
	reloadPayload := map[string]interface{}{"timestamp": time.Now().Unix()}
	// Publish to the topic. The broker will send it to subscribed clients (our hotreload.js client).
	// The JS client should subscribe to TopicHotReload.
	err := hr.broker.Publish(context.Background(), TopicHotReload, reloadPayload)
	if err != nil {
		hr.logger.Error("Failed to publish hot reload command to broker", "error", err)
	}
}

// handleClientError handles client error reports
func (hr *HotReload) handleClientError(clientID string, payload map[string]interface{}) {
	hr.logger.Info("Received client error", "client_id", clientID, "error", payload)

	// Call custom error handler if provided
	if hr.options.ErrorHandler != nil {
		hr.options.ErrorHandler(clientID, payload)
	}

	// Update client info
	hr.clientsMu.Lock()
	defer hr.clientsMu.Unlock()

	client, ok := hr.clients[clientID]
	if !ok {
		// If client wasn't known (e.g. not "ready" yet), create a basic entry
		// Or, you might choose to only log errors for "ready" clients
		client = &clientInfo{
			id:       clientID,
			errors:   []clientError{},
			status:   "connected_with_error", // A status indicating it's connected but maybe not fully ready
			lastSeen: time.Now(),
		}
		// Get client handle to fetch URL if possible (best effort)
		if ch, err := hr.broker.GetClient(clientID); err == nil {
			client.url = ch.ClientURL()
		}
		hr.clients[clientID] = client
	} else {
		// Client known, just update lastSeen
		client.lastSeen = time.Now()
	}

	// Add the error
	newError := clientError{
		message:   getStringOrDefault(payload, "message", "Unknown error"),
		filename:  getStringOrDefault(payload, "filename", ""),
		lineno:    getIntOrDefault(payload, "lineno", 0),
		colno:     getIntOrDefault(payload, "colno", 0),
		stack:     getStringOrDefault(payload, "stack", ""),
		timestamp: getStringOrDefault(payload, "timestamp", time.Now().Format(time.RFC3339)),
	}
	client.errors = append(client.errors, newError)

	// Limit the number of errors stored
	if len(client.errors) > hr.options.MaxErrorsPerClient {
		client.errors = client.errors[len(client.errors)-hr.options.MaxErrorsPerClient:]
	}
}

// handleClientReady handles client ready notifications
func (hr *HotReload) handleClientReady(clientID string, payload map[string]interface{}) {
	hr.logger.Info("Client ready", "client_id", clientID, "payload", payload)

	// Update client info
	hr.clientsMu.Lock()
	defer hr.clientsMu.Unlock()

	client, ok := hr.clients[clientID]
	if !ok {
		client = &clientInfo{
			id:       clientID,
			errors:   []clientError{},
			lastSeen: time.Now(),
		}
		hr.clients[clientID] = client
	}

	client.status = "ready"
	client.lastSeen = time.Now()

	// Update URL if provided in payload
	if url, ok := payload["url"].(string); ok {
		client.url = url
	} else if client.url == "" { // If URL not in payload and not set, try to get from broker ClientHandle
		if ch, err := hr.broker.GetClient(clientID); err == nil {
			client.url = ch.ClientURL()
		}
	}
}

// Helper functions for type conversion
func getStringOrDefault(m map[string]interface{}, key, defaultValue string) string {
	if val, ok := m[key].(string); ok {
		return val
	}
	return defaultValue
}

func getIntOrDefault(m map[string]interface{}, key string, defaultValue int) int {
	// JSON numbers are often float64 when unmarshalled into interface{}
	if val, ok := m[key].(float64); ok {
		return int(val)
	}
	if val, ok := m[key].(int); ok { // Handle if it's already int
		return val
	}
	return defaultValue
}

```