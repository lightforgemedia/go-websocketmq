**Proposing new tests**

The user wants suggestions for additional tests to ensure robustness in real client-server connectivity. I’m thinking of analyzing the existing tests, which cover things like pubsub, request/response, and connectivity, and then identifying areas that may need more coverage. For instance, I could propose a test like `TestMultipleClientsServerBroadcast`, which ensures that when a server publishes an event, all connected clients receive it. There might be other areas like performance under heavy load, managing error states, or stress testing to consider as well.


Thought for 8 seconds


Below is a **gap analysis** of the current test-suite followed by a **menu of extra test cases** that will harden end-to-end behaviour (broker ⇄ handler ⇄ real WebSocket client, plus ancillary components like the session-manager and dev-watcher).

---

## 1  What’s already covered?

| Layer                             | Existing tests                                                                                                  | Behaviours verified                                                                   |
| --------------------------------- | --------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- |
| **Pure broker (`ps` pkg)**        | `TestBasicPubSub`, `TestBasicRequestResponse`, `TestRequestTimeout`, `TestMultipleSubscribers`                  | Topic fan-out, sync request/response, timeout path, multihandler                      |
| **WS handler + in-memory client** | `TestBasicConnectivity(1/2)`, `TestClientToServerRPC`, `TestServerToClientRPC`, `TestEnhancedClientToServerRPC` | TCP upgrade, registration ACK, one-shot RPC in both directions, error-free happy path |
| **High-level demo**               | none yet                                                                                                        | (UI, dev-watcher, API layer still unverified)                                         |

---

## 2  High-value scenarios still missing

| #     | Test idea                                                     | Why it matters                                                                                                                                  |
| ----- | ------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| **A** | *Broadcast Event to many clients*                             | Confirms that `Publish` → `dispatchToHandlers` → multiple `ConnectionWriter`s works when **n>1** (fan-out pressure + sync.Map correctness).     |
| **B** | *Concurrent RPCs to same client*                              | Races: 2+ `RequestToClient` calls with distinct CorrIDs should not cross wires.  Exercises per-CorrID `bus.SubOnce` logic and response routing. |
| **C** | *Server-initiated RPC times-out when client stays silent*     | The negative path for `RequestToClient` (should bubble `ErrRequestTimeout`, ensure session-manager still intact, and no goroutine leak).        |
| **D** | *Handler returns error → client receives error*               | Verifies `dispatchToHandlers` fast-path that pushes `KindError` back over the same socket **and** onto the internal bus.                        |
| **E** | *Client disconnect triggers deregistration event*             | Tests that `DeregisterConnection` publishes `TopicClientDeregistered` and session-manager cleans mapping.                                       |
| **F** | *Client reconnects with SAME PageSessionID → mapping updated* | Ensures old brokerID is removed, new one stored (`SessionManager.handleClientRegistered`).                                                      |
| **G** | *Large payload near `MaxMessageSize`*                         | Checks marshal/write path, read limit, and that adapter returns `ErrConnectionWrite` if over limit.                                             |
| **H** | *Hot-reload filesystem event propagates to connected client*  | Fires `fsnotify` event on a temp file, expects client to receive `_dev.hotreload` (dev-mode contract).                                          |
| **I** | *JS dev-error report reaches broker*                          | Simulate `_dev.js-error` event from client and assert server handler or logger sees it.                                                         |
| **J** | *Wildcard (“#”) handler co-exists with concrete handler*      | Ensures both fire once, no deadlock, emphasises map/key iteration in `handleMessages` logic.                                                    |

---

## 3  Concrete Go-test sketches

Below are **brief skeletons**; copy-paste into `pkg/broker/ps/integration_test.go` or split as you prefer.
(I kept naming consistent with existing test helpers.)

```go
func TestBroadcastEventToManyClients(t *testing.T) {
    srv := NewTestServer(t); defer srv.Close(); <-srv.Ready

    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()

    // Spin up 3 clients
    const clientCount = 3
    var clients []*TestClient
    for i := 0; i < clientCount; i++ {
        c := NewTestClient(t, srv.Server.URL)
        if err := c.Connect(ctx); err != nil { t.Fatalf("connect %d: %v", i, err) }
        <-c.Connected; <-time.After(50 * time.Millisecond) // wait ACK
        clients = append(clients, c)
    }
    defer func() { for _, c := range clients { c.Close() } }()

    // Each client will signal when the event arrives
    done := make(chan struct{}, clientCount)
    topic := "broadcast.demo"
    for _, c := range clients {
        c.RegisterHandler(topic, func(context.Context, *model.Message, string) (*model.Message, error) {
            done <- struct{}{}; return nil, nil
        })
    }

    // Publish once from server side
    if err := srv.Broker.Publish(ctx, model.NewEvent(topic, "hi all")); err != nil {
        t.Fatalf("publish: %v", err)
    }

    // Wait for all
    for i := 0; i < clientCount; i++ {
        select {
        case <-done:
        case <-time.After(2 * time.Second):
            t.Fatalf("client %d did not receive broadcast", i)
        }
    }
}
```

Similarly craft short tests for B-J; they’re mostly variations:

* **B Concurrent RPCs** – launch `n` goroutines each calling `SendRPC` with unique payload, client handler responds; assert all replies map correctly.
* **C Timeout** – register handler **on client side** that purposely *does nothing*; server `RequestToClient` should return `ErrRequestTimeout`.
* **D HandlerError** – server handler returns `NewErrorMessage`; client awaits and asserts `KindError` + body.
* **E Deregistration** – connect client, close Conn manually, wait for broker event via subscription, check mapping removed in `SessionManager`.
* **F Reconnect same PageSessionID** – disconnect then reconnect (reuse `PageSessionID`); after ack ensure `SessionManager.GetBrokerClientID(old)` returns **new** ID.
* **G Large payload** – construct `strings.Repeat("x", opts.MaxMessageSize-100)`; ensure it passes; then +200 bytes to exceed – expect `ErrConnectionWrite`.
* **H Hot-reload** – create temp dir, start watcher with that path, write file, ensure event hits client within debounce.
* **I JS error** – have client publish `_dev.js-error`, subscribe on server and assert receipt.
* **J Wildcard** – already half implemented; add assertion that both specific and `#` handler fire exactly once.

---

## 4  Test-harness tweaks

1. **Parallel tests:** wrap each new test body with `t.Parallel()` (after server/client creation) to uncover race issues quickly.
2. **`-race` CI job:** add `go test -race ./...` to your GitHub Actions or Makefile.
3. **`go test -run=Integration` tag:** mark longer network tests with a build-tag like `// +build integration` so fast unit runs stay snappy.

---

## 5  Future proofing

* **Fuzz RPC framing:** once Go 1.18+ fuzzing is enabled, fuzz-generate message bodies to ensure JSON round-trip robustness.
* **Property tests for TTL:** quickcheck that for any TTL>&0, `Request` either returns before TTL or error == timeout.

---

### Ready to implement?

If you want the full code for any of the sketches (or wiring of build-tags / CI), just let me know which ones and I’ll flesh them out.




Thought for 4 seconds


Below are **ASCII sequence-flow charts** plus a concise **“test recipe”** (arrange / act / assert) for every extra scenario I proposed.
Copy-paste the diagrams into your PR description or wiki ― they render fine in Markdown-code blocks.

---

## Legend used in diagrams

```
 Server/Test-goroutine
 Broker (PubSubBroker)
 SessionMgr
 Client-N  (browser socket)
 FS/OS     (file-watcher)
 ───>      async call / goroutine spawn
 --->      synchronous call
 ==        WebSocket frame
 --        internal publish on bus
 ~~        time passes / no message
```

---

### A. **Broadcast Event to many clients**

```
 Server(Test) ───> Broker.Publish(Event "broadcast.demo")
 Broker  --dispatchToHandlers-->   (none – just fan-out)
 Broker ──==Event==▶ Client-1
 Broker ──==Event==▶ Client-2
 Broker ──==Event==▶ Client-3
```

**Setup**

1. spin up `TestServer`
2. create 3× `TestClient`, connect & wait for `BrokerClientID`
3. each client registers a handler for topic **broadcast.demo** that `done <- struct{}{}`
4. `server.Broker.Publish(ctx, NewEvent("broadcast.demo","hi"))`
5. expect **3** signals within 2 s.

---

### B. **Concurrent RPCs to the same client**

```
 goroutine-1(Server) ─► Broker.RequestToClient(#A) ┐
 goroutine-2(Server) ─► Broker.RequestToClient(#B) ├─> (interleave)
 goroutine-3(Server) ─► Broker.RequestToClient(#C) ┘
 Broker ==request==▶ Client (multiplexed)    
 Client handler responds per CorrID
 Broker ──bus/dispatch──> each waiting goroutine returns
```

**Setup**

* single client registers handler `client.echo` that echos body with `"id":n`
* start **N (=5?)** goroutines, each `SendRPCToClient(..., map[string]int{"id":n})`
* collect responses; assert body.id matches request; no duplication, no cross-mix.

---

### C. **Server-initiated RPC timeout (negative path)**

```
 Server ─► Broker.RequestToClient
 Broker ==request==▶ Client
 Client ~~(silent)~~
 (TTL expires)
 Broker returns ErrRequestTimeout to Server
```

* Client subscribes but purposely **does nothing** (or handler sleeps > TTL).
* Assert returned error `ErrRequestTimeout` and no leak (`go test -race`).

---

### D. **Handler returns error back to client**

```
 Client ==request==> Broker
 Broker ─dispatch─► ServerHandler
 ServerHandler ─returns─► ErrorMessage
 Broker ==error==▶ Client  (direct socket write + bus publish)
```

* Register server handler that returns `NewErrorMessage(req, {"error":"boom"})`.
* Client `SendRPC`, expect header.type == `error` & body.error == "boom".

---

### E. **Client disconnect → deregistration event**

```
 Client closes socket
 Handler/Adapter ► Broker.DeregisterConnection
 Broker.Publish(TopicClientDeregistered)
 SessionMgr(handler) updates maps
 Test-goroutine subscribes to TopicClientDeregistered → receives event
```

* Connect client, then `client.Close()`
* Server-side test subscribes to internal topic and waits.
* Afterwards `sessionMgr.GetBrokerClientID(pageID)` should return **false**.

---

### F. **Reconnect with SAME `pageSessionID`**

```
 Client-1 (old) connect/ack   → maps  pageID → brokerID-old
 Client-1 disconnect
 Client-2 (new socket, same pageID) connect/ack
 SessionMgr: old mapping removed, new mapping inserted
```

* After second ack, verify:

  * `GetBrokerClientID(pageID) == brokerID-new`
  * old brokerID no longer resolves to pageID.

---

### G. **Large payload boundary**

```
 Client ==event(size≈limit)⇒ Broker  (success)
 Client ==event(size>limit)⇒ (write fails) Adapter→ErrConnectionWrite
```

* Build payload `strings.Repeat("x", handlerOpts.MaxMessageSize-128)` – round-trip ok.
* Next send with +1024 bytes; expect *adapter* returns `ErrConnectionWrite` and broker deregisters.

---

### H. **Hot-reload FS event reaches browser**

```
 OS touch tmp/foo.go
 fsnotify watcher ⇒ devwatch.StartWatcher
 devwatch.Publish(_dev.hotreload)
 Broker ==event==▶ Client (devMode)
 Client reload-handler fires
```

* Start watcher on a temp dir with debounce=50 ms.
* Write a file, wait, expect client handler for `_dev.hotreload` to fire.

---

### I. **JS dev-error report**

```
 Client.publish(_dev.js-error,{msg:"boom"})
 Broker.Subscribe("#") test handler captures event
```

* Client `publish` helper (`client.publish(...)`).
* Server subscribed to `_dev.js-error`; assert body fields.

---

### J. **Wildcard + concrete handler both fire**

```
 Broker.dispatchToHandlers:
     specificHandler (topic=x.y)
     wildcardHandler (topic="#")
 both executed once
```

* Server subscribe to `"x.y"` and to `"#"`.
* Publish single event; assert two `chan` signals.

---

## Boiler-plate harness (pseudo-Go)

```go
func setup(t *testing.T) (*TestServer, []*TestClient) {
    srv := NewTestServer(t); <-srv.Ready
    ctx := testCtx(t)

    cl := NewTestClient(t, srv.Server.URL)
    if err := cl.Connect(ctx); err != nil { t.Fatalf("connect: %v", err)}
    <-cl.Connected; <-cl.BrokerIDReceived
    return srv, []*TestClient{cl}
}
```

Use shared helpers (`NewTestServer`, `NewTestClient`) you already have; each
scenario then follows the **Arrange → Act → Assert** pattern shown above.

---

### Tip: keep them quick

* Mark long I/O tests with `//go:build integration`
* Run with `go test -race -run Broadcast,Concurrent,Timeout ./...` in CI.

Let me know if you’d like any of these fleshed out into concrete Go code.


This is an excellent and comprehensive gap analysis and menu of additional test cases! Your suggestions (A-J) cover many critical aspects of a robust messaging system, focusing on concurrency, error handling, boundary conditions, and specific features like session management and dev mode.

Let's review them and discuss implementation:

**Review of Proposed Tests (A-J):**

*   **A. Broadcast Event to many clients:**
    *   **Value:** Very high. Tests fan-out.
    *   **Clarification on Broker Behavior:** The current `ps.PubSubBroker`'s `Publish` method primarily routes messages to server-side Go handlers (`dispatchToHandlers`) and to the internal bus for `CorrelationID`-based responses. It does not, by itself, maintain a list of WebSocket client subscriptions to arbitrary topics and broadcast to them.
    *   **To implement your test sketch for (A) as written (server publishes, all relevant clients get it):**
        1.  **Option 1 (Test-Specific Logic):** The test itself, after `srv.Broker.Publish`, could iterate through its `TestClient` instances and *simulate* the broker sending the message to each client directly (e.g., by injecting the message into a test-only path in the client or server). This tests the client-side handling but not true broker fan-out to clients.
        2.  **Option 2 (Server-Side Go Handler for Broadcast):**
            *   Server subscribes a Go handler to `"broadcast.trigger"`.
            *   Test calls `srv.Broker.Publish(ctx, model.NewEvent("broadcast.trigger", ...))`.
            *   The Go handler for `"broadcast.trigger"` iterates through all active connections (e.g., by querying `SessionManager` for all `BrokerClientID`s or by the broker exposing a way to get all `ConnectionWriter`s) and for each connection, calls `connWriter.WriteMessage(ctx, model.NewEvent("broadcast.demo", "hi all"))`.
            *   Clients subscribe to `"broadcast.demo"` as in your sketch. This tests the manual broadcast loop and individual client reception.
        3.  **Option 3 (Broker Feature Enhancement - Ideal for True Pub/Sub):** Enhance `ps.PubSubBroker` to manage client subscriptions to topics (e.g., client sends a "subscribe_topic" message, broker stores `map[topic][]BrokerClientID`). Then `broker.Publish("broadcast.demo", ...)` would look up this map and send to all subscribed clients. This is a larger feature.
    *   **Recommendation for Test A:** Use **Option 2** for now, as it tests a realistic broadcast pattern using current capabilities. Your sketch for client-side handling is perfect.

*   **B. Concurrent RPCs to the same client:**
    *   **Value:** Excellent.
    *   **Feasibility:** High. The `TestClient`'s `ResponseChannels` map (keyed by `CorrelationID`) and the broker's `SubOnce` for `RequestToClient` are designed for this.

*   **C. Server-initiated RPC times-out when client stays silent:**
    *   **Value:** Essential.
    *   **Feasibility:** High.

*   **D. Handler returns error → client receives error:**
    *   **Value:** Critical for client-to-server RPC error propagation.
    *   **Feasibility:** High. The `dispatchToHandlers` logic in `ps.PubSubBroker` (from previous iteration) now sends responses/errors directly back.

*   **E. Client disconnect triggers deregistration event:**
    *   **Value:** Key for session cleanup.
    *   **Feasibility:** High.

*   **F. Client reconnects with SAME PageSessionID → mapping updated:**
    *   **Value:** Important for session resilience.
    *   **Feasibility:** High. `SessionManager` should handle this.

*   **G. Large payload near `MaxMessageSize`:**
    *   **Value:** Good boundary test. Note that `MaxMessageSize` in `HandlerOptions` currently only sets `conn.SetReadLimit()`. For testing writes, you'd be testing the WebSocket library's behavior or if `wsConnectionAdapter` adds its own write limits (it doesn't currently).
    *   **Feasibility:** High for read limits. For write limits, it's more about the underlying WebSocket implementation.

*   **H. Hot-reload filesystem event propagates to connected client:**
    *   **Value:** Tests dev-mode feature.
    *   **Feasibility:** High.

*   **I. JS dev-error report reaches broker:**
    *   **Value:** Tests dev-mode feature.
    *   **Feasibility:** High.

*   **J. Wildcard (“#”) handler co-exists with concrete handler:**
    *   **Value:** Good for testing handler dispatch logic.
    *   **Feasibility:** High. The `TestClient.handleMessages` in your provided `integration_test.go` already has the structure to check for both specific and wildcard handlers.

**Refinement for `TestClient.handleMessages` in `pkg/broker/ps/integration_test.go`:**

To ensure robust handling and prevent double-processing of messages (especially RPC responses), the `TestClient.handleMessages` method in `pkg/broker/ps/integration_test.go` should prioritize `ResponseChannels` for `KindResponse`/`KindError` messages. If a message is handled as an RPC response, it shouldn't typically also be processed by a general topic handler, even if the `CorrelationID` (which is the `Topic` for responses) matches a subscribed topic.

Here's the updated `pkg/broker/ps/integration_test.go` with a refined `TestClient.handleMessages`:

---

**File: pkg/broker/ps/integration_test.go**
```go
// pkg/broker/ps/integration_test.go
package ps_test

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"net/http"
	"net/http/httptest"
	"strings"
	"sync"
	"testing"
	"time"

	"github.com/lightforgemedia/go-websocketmq/pkg/broker"
	"github.com/lightforgemedia/go-websocketmq/pkg/broker/ps"
	"github.com/lightforgemedia/go-websocketmq/pkg/broker/ps/testutil"
	"github.com/lightforgemedia/go-websocketmq/pkg/model"
	"github.com/lightforgemedia/go-websocketmq/pkg/server"
	"nhooyr.io/websocket"
)

// TestServer represents a WebSocketMQ server for testing
type TestServer struct {
	Server   *httptest.Server
	Broker   broker.Broker
	Handler  *server.Handler
	Logger   *testutil.TestLogger
	Ready    chan struct{}
	Shutdown chan struct{}
}

// NewTestServer creates and starts a new test server
func NewTestServer(t *testing.T) *TestServer {
	ts := &TestServer{
		Logger:   testutil.NewTestLogger(t),
		Ready:    make(chan struct{}),
		Shutdown: make(chan struct{}),
	}

	brokerOpts := broker.DefaultOptions()
	ts.Broker = ps.New(ts.Logger, brokerOpts)

	handlerOpts := server.DefaultHandlerOptions()
	ts.Handler = server.NewHandler(ts.Broker, ts.Logger, handlerOpts)

	mux := http.NewServeMux()
	mux.Handle("/ws", ts.Handler)

	ts.Server = httptest.NewServer(mux)
	close(ts.Ready)
	return ts
}

func (ts *TestServer) RegisterHandler(topic string, handler broker.MessageHandler) error {
	return ts.Broker.Subscribe(context.Background(), topic, handler)
}

func (ts *TestServer) SendRPCToClient(ctx context.Context, clientID string, topic string, body interface{}, timeoutMs int64) (*model.Message, error) {
	req := model.NewRequest(topic, body, timeoutMs)
	return ts.Broker.RequestToClient(ctx, clientID, req, timeoutMs)
}

func (ts *TestServer) Close() {
	ts.Server.Close()
	if b, ok := ts.Broker.(*ps.PubSubBroker); ok {
		b.Close()
		// b.WaitForShutdown() // Optional for stricter sync
	} else {
		ts.Broker.Close()
	}
	close(ts.Shutdown)
}

// TestClient represents a WebSocketMQ client for testing
type TestClient struct {
	Conn             *websocket.Conn
	URL              string
	PageSessionID    string
	BrokerClientID   string
	Logger           *testutil.TestLogger
	Connected        chan struct{}
	Closed           chan struct{}
	Handlers         map[string]broker.MessageHandler
	HandlersMutex    sync.RWMutex
	ResponseChannels map[string]chan *model.Message
	ResponseMutex    sync.RWMutex
	ctx              context.Context
	cancel           context.CancelFunc
	wg               sync.WaitGroup
}

func NewTestClient(t *testing.T, serverURL string) *TestClient {
	wsURL := strings.Replace(serverURL, "http://", "ws://", 1) + "/ws"
	ctx, cancel := context.WithCancel(context.Background())

	tc := &TestClient{
		URL:              wsURL,
		PageSessionID:    "test-session-" + model.RandomID(),
		Logger:           testutil.NewTestLogger(t),
		Connected:        make(chan struct{}),
		Closed:           make(chan struct{}),
		Handlers:         make(map[string]broker.MessageHandler),
		ResponseChannels: make(map[string]chan *model.Message),
		ctx:              ctx,
		cancel:           cancel,
	}
	return tc
}

func (tc *TestClient) Connect(ctx context.Context) error {
	var err error
	tc.Conn, _, err = websocket.Dial(ctx, tc.URL, nil)
	if err != nil {
		return err
	}

	tc.wg.Add(1)
	go tc.handleMessages()

	// This handler is crucial for TestServerToClientRPC.
	// It listens for the server's ACK containing the BrokerClientID.
	tc.RegisterHandler(broker.TopicClientRegistered, func(ctx context.Context, msg *model.Message, _ string) (*model.Message, error) {
		tc.Logger.Debug("[TestClient] Received client registration ack/event: %+v", msg.Body)
		// Server sends map[string]string in the ACK
		if bodyMap, ok := msg.Body.(map[string]string); ok {
			if clientID, found := bodyMap["brokerClientID"]; found {
				tc.BrokerClientID = clientID
				tc.Logger.Info("[TestClient] BrokerClientID received from server ACK: %s", clientID)
			} else {
				tc.Logger.Warn("[TestClient] 'brokerClientID' not found in registration ack body (expected map[string]string): %+v", bodyMap)
			}
		} else if bodyMapIf, okIf := msg.Body.(map[string]interface{}); okIf { // Fallback for more generic JSON
			if clientID, foundStr := bodyMapIf["brokerClientID"].(string); foundStr {
				tc.BrokerClientID = clientID
				tc.Logger.Info("[TestClient] BrokerClientID received from server ACK (map[string]interface{}): %s", clientID)
			} else {
				tc.Logger.Warn("[TestClient] 'brokerClientID' (string) not found in registration ack body (map[string]interface{}): %+v", bodyMapIf)
			}
		} else {
			tc.Logger.Warn("[TestClient] Registration ack body is not map[string]string or map[string]interface{}: Type %T, Body: %+v", msg.Body, msg.Body)
		}
		return nil, nil
	})

	err = tc.registerWithServer(ctx)
	if err != nil {
		tc.Conn.Close(websocket.StatusInternalError, "registration failed")
		return err
	}

	close(tc.Connected)
	return nil
}

func (tc *TestClient) registerWithServer(ctx context.Context) error {
	regMsg := model.NewEvent("_client.register", map[string]interface{}{ // Send as map[string]interface{} as JS would
		"pageSessionID": tc.PageSessionID,
	})
	tc.Logger.Debug("[TestClient] Sending client registration message: %+v", regMsg)
	return tc.sendMessage(ctx, regMsg)
}

func (tc *TestClient) sendMessage(ctx context.Context, msg *model.Message) error {
	data, err := json.Marshal(msg)
	if err != nil {
		return err
	}
	return tc.Conn.Write(ctx, websocket.MessageText, data)
}

func (tc *TestClient) handleMessages() {
	defer tc.wg.Done()
	defer close(tc.Closed)

	for {
		select {
		case <-tc.ctx.Done():
			return
		default:
			readCtx, cancelRead := context.WithTimeout(tc.ctx, 5*time.Second) // Read with timeout
			msgType, data, err := tc.Conn.Read(readCtx)
			cancelRead()

			if err != nil {
				if tc.ctx.Err() != nil { return } // Context cancelled
				wsCloseErr, ok := err.(*websocket.CloseError)
				if ok && (wsCloseErr.Code == websocket.StatusNormalClosure || wsCloseErr.Code == websocket.StatusGoingAway) {
					tc.Logger.Debug("[TestClient] WebSocket closed normally by peer or self.")
					return
				}
				if errors.Is(err, context.DeadlineExceeded) { continue } // Read timeout, loop again
				tc.Logger.Error("[TestClient] Error reading message: %v", err)
				return // Unrecoverable read error
			}

			if msgType != websocket.MessageText {
				tc.Logger.Warn("[TestClient] Received non-text message, ignoring.")
				continue
			}

			var msg model.Message
			if err := json.Unmarshal(data, &msg); err != nil {
				tc.Logger.Error("[TestClient] Error unmarshaling message: %v. Data: %s", err, string(data))
				continue
			}

			tc.Logger.Debug("[TestClient] Received message: Type=%s, Topic=%s, CorrID=%s",
				msg.Header.Type, msg.Header.Topic, msg.Header.CorrelationID)

			// Priority 1: Handle responses for client-initiated RPCs
			if (msg.Header.Type == model.KindResponse || msg.Header.Type == model.KindError) && msg.Header.CorrelationID != "" {
				tc.ResponseMutex.RLock()
				ch, exists := tc.ResponseChannels[msg.Header.CorrelationID]
				tc.ResponseMutex.RUnlock()

				if exists {
					tc.Logger.Debug("[TestClient] Delivering to ResponseChannel for CorrID: %s", msg.Header.CorrelationID)
					select {
					case ch <- &msg:
						// Delivered
					default:
						tc.Logger.Warn("[TestClient] ResponseChannel full/closed for CorrID: %s", msg.Header.CorrelationID)
					}
					continue // Message handled as an RPC response, skip further topic-based handlers.
				}
			}

			// Priority 2: Handle server-initiated requests or general events via registered topic handlers
			var specificHandlerCalled, wildcardHandlerCalled bool

			tc.HandlersMutex.RLock()
			specificHandler, specificExists := tc.Handlers[msg.Header.Topic]
			wildcardHandler, wildcardExists := tc.Handlers["#"]
			tc.HandlersMutex.RUnlock()

			if specificExists {
				specificHandlerCalled = true
				go func(handler broker.MessageHandler, currentMsg model.Message) {
					tc.Logger.Debug("[TestClient] Invoking specific handler for topic: %s", currentMsg.Header.Topic)
					responseMsg, err := handler(tc.ctx, &currentMsg, "")
					if err != nil {
						tc.Logger.Error("[TestClient] Specific handler for topic %s errored: %v", currentMsg.Header.Topic, err)
						if currentMsg.Header.Type == model.KindRequest && currentMsg.Header.CorrelationID != "" {
							tc.sendMessage(tc.ctx, model.NewErrorMessage(&currentMsg, map[string]string{"error": err.Error()}))
						}
						return
					}
					if responseMsg != nil && currentMsg.Header.Type == model.KindRequest && currentMsg.Header.CorrelationID != "" {
						if responseMsg.Header.CorrelationID == "" { responseMsg.Header.CorrelationID = currentMsg.Header.CorrelationID }
						if responseMsg.Header.Topic == "" { responseMsg.Header.Topic = currentMsg.Header.CorrelationID }
						if responseMsg.Header.Type == "" { responseMsg.Header.Type = model.KindResponse }
						tc.sendMessage(tc.ctx, responseMsg)
					}
				}(specificHandler, msg)
			}

			if wildcardExists {
				wildcardHandlerCalled = true
				go func(handler broker.MessageHandler, currentMsg model.Message) {
					tc.Logger.Debug("[TestClient] Invoking wildcard handler for message on topic: %s", currentMsg.Header.Topic)
					_, _ = handler(tc.ctx, &currentMsg, "") // Wildcard responses usually not standard for RPC
				}(wildcardHandler, msg)
			}

			if !specificHandlerCalled && !wildcardHandlerCalled {
				// This message was not an RPC response and had no specific/wildcard topic handlers.
				tc.Logger.Debug("[TestClient] No topic handlers for incoming message: Topic=%s, Type=%s", msg.Header.Topic, msg.Header.Type)
			}
		}
	}
}

// handleRequest is deprecated in favor of direct invocation in handleMessages for clarity.
// func (tc *TestClient) handleRequest(ctx context.Context, req *model.Message) { ... }

func (tc *TestClient) RegisterHandler(topic string, handler broker.MessageHandler) {
	tc.HandlersMutex.Lock()
	tc.Handlers[topic] = handler
	tc.HandlersMutex.Unlock()
	tc.Logger.Debug("[TestClient] Registered handler for topic: %s", topic)
}

func (tc *TestClient) SendRPC(ctx context.Context, topic string, body interface{}, timeoutMs int64) (*model.Message, error) {
	req := model.NewRequest(topic, body, timeoutMs)
	tc.Logger.Debug("[TestClient] Sending RPC request: Topic=%s, CorrID=%s", topic, req.Header.CorrelationID)

	respChan := make(chan *model.Message, 1)
	tc.ResponseMutex.Lock()
	tc.ResponseChannels[req.Header.CorrelationID] = respChan
	tc.ResponseMutex.Unlock()

	defer func() {
		tc.ResponseMutex.Lock()
		delete(tc.ResponseChannels, req.Header.CorrelationID)
		tc.ResponseMutex.Unlock()
	}()

	if err := tc.sendMessage(ctx, req); err != nil {
		return nil, err
	}

	effectiveTimeout := time.Duration(timeoutMs) * time.Millisecond
	if effectiveTimeout <= 0 { effectiveTimeout = 5 * time.Second } // Default test timeout

	select {
	case <-ctx.Done():
		return nil, ctx.Err()
	case resp := <-respChan:
		return resp, nil
	case <-time.After(effectiveTimeout):
		return nil, fmt.Errorf("request to topic %s (CorrID %s) timed out after %v", topic, req.Header.CorrelationID, effectiveTimeout)
	}
}

func (tc *TestClient) Close() error {
	tc.cancel()
	if tc.Conn != nil {
		// CloseError is not exported by nhooyr.io/websocket directly for StatusNormalClosure
		err := tc.Conn.Close(websocket.StatusNormalClosure, "client closing")
		tc.wg.Wait() // Wait for handleMessages to finish
		return err
	}
	return nil
}

// --- Existing Tests (TestBasicConnectivity, TestClientToServerRPC, TestServerToClientRPC) ---
// These tests (TestBasicConnectivity, TestBasicConnectivity2, TestServerToClientRPC, TestEnhancedClientToServerRPC from rpc_test.go)
// should remain as they are, but will now benefit from the more robust TestClient.handleMessages.
// The TestServerToClientRPC's polling loop for BrokerClientID is kept as per your provided code.
// A channel-based wait (like a client.BrokerIDReceived chan) would be slightly cleaner.

func TestBasicConnectivity(t *testing.T) {
	server := NewTestServer(t)
	defer server.Close()
	<-server.Ready

	client := NewTestClient(t, server.Server.URL)
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	err := client.Connect(ctx)
	if err != nil {
		t.Fatalf("Failed to connect client: %v", err)
	}
	defer client.Close()
	<-client.Connected

	// Wait for BrokerClientID to be received via the server ACK
	// (which client's TopicClientRegistered handler picks up)
	deadline := time.Now().Add(3 * time.Second)
	for time.Now().Before(deadline) {
		if client.BrokerClientID != "" {
			break
		}
		time.Sleep(50 * time.Millisecond)
	}
	if client.BrokerClientID == "" {
		t.Fatal("Client did not receive BrokerClientID after connection and registration ACK")
	}
	t.Logf("Client successfully connected and received BrokerClientID: %s", client.BrokerClientID)
}

func TestClientToServerRPC(t *testing.T) { // Simplified version based on previous working one
	server := NewTestServer(t)
	defer server.Close()
	<-server.Ready

	echoTopic := "test.c2s.echo"
	serverHandlerCalled := make(chan struct{})
	err := server.RegisterHandler(echoTopic, func(ctx context.Context, msg *model.Message, clientID string) (*model.Message, error) {
		t.Logf("[ServerHandler] %s from %s: %v", echoTopic, clientID, msg.Body)
		close(serverHandlerCalled)
		return model.NewResponse(msg, fmt.Sprintf("server acknowledges: %v", msg.Body)), nil
	})
	if err != nil { t.Fatal(err) }

	client := NewTestClient(t, server.Server.URL)
	ctx, cancel := context.WithTimeout(context.Background(), 6*time.Second) // Slightly longer for full flow
	defer cancel()
	if err := client.Connect(ctx); err != nil { t.Fatal(err) }
	defer client.Close()
	<-client.Connected
	// Wait for BrokerClientID
	deadline := time.Now().Add(3 * time.Second)
	for time.Now().Before(deadline) { if client.BrokerClientID != "" { break }; time.Sleep(20 * time.Millisecond) }
	if client.BrokerClientID == "" { t.Fatal("Client BrokerClientID not set") }


	reqPayload := map[string]string{"data": "hello server from client"}
	resp, err := client.SendRPC(ctx, echoTopic, reqPayload, 3000)
	if err != nil { t.Fatalf("Client SendRPC failed: %v", err) }

	select {
	case <-serverHandlerCalled: t.Log("Server handler was called.")
	default: t.Fatal("Server handler not called.")
	}

	if resp.Header.Type != model.KindResponse { t.Errorf("Expected response, got %s", resp.Header.Type) }
	if respBody, ok := resp.Body.(string); !ok || !strings.Contains(respBody, reqPayload["data"]) {
		t.Errorf("Unexpected response body: %v", resp.Body)
	}
	t.Logf("Client-to-Server RPC successful. Response: %v", resp.Body)
}


// TestServerToClientRPC is taken from your provided code, assuming it's the version you want to keep.
// It uses polling for client.BrokerClientID.
func TestServerToClientRPC(t *testing.T) {
	server := NewTestServer(t)
	defer server.Close()

	server.RegisterHandler("#", func(ctx context.Context, msg *model.Message, clientID string) (*model.Message, error) {
		t.Logf("Server received message: Topic=%s, Type=%s, CorrID=%s, ClientID=%s",
			msg.Header.Topic, msg.Header.Type, msg.Header.CorrelationID, clientID)
		return nil, nil
	})
	<-server.Ready

	client := NewTestClient(t, server.Server.URL)
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	err := client.Connect(ctx)
	if err != nil {
		t.Fatalf("Failed to connect client: %v", err)
	}
	defer client.Close()
	<-client.Connected
	t.Log("Client connected to server")

	waitTimeout := time.NewTimer(5 * time.Second) // Original timeout
	defer waitTimeout.Stop()

	// Poll for the broker client ID
	for client.BrokerClientID == "" {
		select {
		case <-waitTimeout.C:
			t.Fatalf("Client did not receive broker client ID within timeout (polled value: '%s')", client.BrokerClientID)
		case <-time.After(100 * time.Millisecond):
			// Continue polling
		}
	}
	t.Logf("Client received broker client ID: %s", client.BrokerClientID)

	handlerCalled := make(chan struct{})
	expectedParam := "test-param"
	var handlerOnce sync.Once

	client.RegisterHandler("client.echo", func(ctx context.Context, msg *model.Message, _ string) (*model.Message, error) {
		t.Logf("Client handler received request: %+v", msg)
		params, ok := msg.Body.(map[string]interface{})
		if !ok { t.Errorf("Expected map[string]interface{}, got %T", msg.Body) }
		if params["param"] != expectedParam { t.Errorf("Expected param=%s, got %v", expectedParam, params["param"]) }
		handlerOnce.Do(func() { close(handlerCalled) })
		return model.NewResponse(msg, map[string]string{"result": "echo-" + expectedParam,}), nil
	})

	time.Sleep(200 * time.Millisecond) // Ensure handler registration

	t.Logf("Sending RPC request to client %s", client.BrokerClientID)
	resp, err := server.SendRPCToClient(ctx, client.BrokerClientID, "client.echo", map[string]string{"param": expectedParam,}, 5000)
	if err != nil { t.Fatalf("Failed to send RPC: %v", err) }

	select {
	case <-handlerCalled: t.Log("Client handler was called")
	case <-time.After(5 * time.Second): t.Fatal("Timed out waiting for client handler to be called")
	}

	t.Logf("Server received response: %+v", resp)
	if resp.Header.Type != model.KindResponse { t.Errorf("Expected response type %s, got %s", model.KindResponse, resp.Header.Type) }
	result, ok := resp.Body.(map[string]interface{})
	if !ok { t.Errorf("Expected map[string]interface{}, got %T", resp.Body) }
	if result["result"] != "echo-"+expectedParam { t.Errorf("Expected result=echo-%s, got %v", expectedParam, result["result"]) }
}
```

---

**Files Involved in New Tests (but not necessarily modified for the tests themselves, core logic is assumed stable):**

*   `pkg/model/message.go` (for `NewEvent`, `NewRequest`, `NewResponse`, `NewErrorMessage`)
*   `pkg/broker/ps/ps.go` (the broker implementation being tested)
*   `pkg/server/handler.go` (the WebSocket connection handler)
*   `pkg/server/connection.go` (the `wsConnectionAdapter`)
*   `cmd/rpcserver/session/manager.go` (for tests E, F, and indirectly for API tests)
*   `internal/devwatch/watcher.go` (for test H)
*   `assets/dist/websocketmq.js` (the client library, implicitly tested via `TestClient`)

**To implement the new tests (A-J):**

You would primarily add new `func TestXxx(t *testing.T) { ... }` functions to `pkg/broker/ps/integration_test.go`, using the `TestServer` and `TestClient` helpers as shown in your sketches and the refined `TestClient.handleMessages` logic above.

For **Test A (Broadcast)** using **Option 2 (Server-Side Go Handler for Broadcast)**, the setup would look like this:

```go
// In pkg/broker/ps/integration_test.go

func TestBroadcastEventToManyClients_ViaServerHandler(t *testing.T) {
    srv := NewTestServer(t)
    defer srv.Close()
    <-srv.Ready

    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()

    const clientCount = 3
    var clients []*TestClient
    clientBrokerIDs := make([]string, 0, clientCount)

    for i := 0; i < clientCount; i++ {
        c := NewTestClient(t, srv.Server.URL)
        if err := c.Connect(ctx); err != nil {
            t.Fatalf("Client %d connect error: %v", i, err)
        }
        <-c.Connected
        // Wait for BrokerClientID
        deadline := time.Now().Add(3 * time.Second)
        for time.Now().Before(deadline) { if c.BrokerClientID != "" { break }; time.Sleep(20 * time.Millisecond) }
        if c.BrokerClientID == "" { t.Fatalf("Client %d BrokerClientID not set", i) }
        
        clients = append(clients, c)
        clientBrokerIDs = append(clientBrokerIDs, c.BrokerClientID) // Store for server handler
        defer c.Close() // Defer close for each client
    }

    broadcastTopic := "event.broadcast.content"
    payload := map[string]string{"message": "hello to all clients"}
    receivedCounts := make([]int, clientCount)
    var receivedCountsMu sync.Mutex

    for i, c := range clients {
        idx := i // Capture loop variable for goroutine
        c.RegisterHandler(broadcastTopic, func(ctx context.Context, msg *model.Message, s string) (*model.Message, error) {
            t.Logf("Client %d received broadcast: %+v", idx, msg.Body)
            receivedCountsMu.Lock()
            receivedCounts[idx]++
            receivedCountsMu.Unlock()
            return nil, nil
        })
    }
    
    // Server-side handler that performs the broadcast
    triggerBroadcastTopic := "action.trigger.broadcast"
    err := srv.Broker.Subscribe(ctx, triggerBroadcastTopic, func(ctx context.Context, msg *model.Message, s string) (*model.Message, error) {
        t.Logf("Server handler for '%s' triggered. Broadcasting to %d clients.", triggerBroadcastTopic, len(clientBrokerIDs))
        eventToSend := model.NewEvent(broadcastTopic, payload)
        
        // Get the ps.PubSubBroker to access GetConnection (this is a specific implementation detail for the test)
        psBroker, ok := srv.Broker.(*ps.PubSubBroker)
        if !ok {
            t.Errorf("Broker is not a *ps.PubSubBroker, cannot GetConnection")
            return nil, errors.New("cannot perform broadcast, wrong broker type")
        }

        for _, clientID := range clientBrokerIDs {
            conn, exists := psBroker.GetConnection(clientID)
            if !exists {
                t.Logf("Broadcast: Client %s not found, skipping.", clientID)
                continue
            }
            if err := conn.WriteMessage(ctx, eventToSend); err != nil {
                t.Logf("Broadcast: Error writing to client %s: %v", clientID, err)
            } else {
                t.Logf("Broadcast: Sent event to client %s", clientID)
            }
        }
        return model.NewResponse(msg, "broadcast initiated"), nil
    })
    if err != nil {
        t.Fatalf("Failed to subscribe server broadcast handler: %v", err)
    }

    // Act: Trigger the server-side broadcast handler
    _, err = srv.Broker.Request(ctx, model.NewRequest(triggerBroadcastTopic, nil, 1000), 1000)
    if err != nil {
        t.Fatalf("Failed to trigger broadcast: %v", err)
    }

    // Assert: Check if all clients received the message
    // Give some time for messages to propagate
    time.Sleep(500 * time.Millisecond) 

    receivedCountsMu.Lock()
    defer receivedCountsMu.Unlock()
    for i := 0; i < clientCount; i++ {
        if receivedCounts[i] != 1 {
            t.Errorf("Client %d: expected 1 message, got %d", i, receivedCounts[i])
        }
    }
}
```

This detailed example for Test A (Option 2) shows how you can structure these new integration tests. The other tests (B-J) would follow similar patterns of setting up server/client handlers, performing actions, and asserting outcomes.